 # Ecological Genomics Course: Lab Notebook    

### Author: Hannah Lachance        

## Overall Description of notebook      

The purpose of this notebook is to keep track of and organize the information that is pertinant to this course. 


## Date started: 2017-01-18    
## Date end:  ongoing   

## Philosophy   
"Science should be reproducible and one of the best ways to achieve this is by logging research activities in a notebook. Because science/biology has increasingly become computational, it is easier to document computational projects in an electronic form, which can be shared online through Github."    

### Helpful features of the notebook     

**It is absolutely critical for your future self and others to follow your work.**     

*  The notebook is set up with a series of internal links from the table of contents.    
*  All notebooks should have a table of contents which has the "Page", date, and title (information that allows the reader to understand your work).     
*  Also, one of the perks of keeping all activities in a single document is that you can **search and find elements quickly**.     
                D* You can document anything you'd like, aside from logging your research activities. For example:
   * feel free to log all/any ideas for your research project([example](https://github.com/adnguyen/Notebooks_and_Protocols/blob/master/2016_notebook.md#page-39-2016-06-13-post-doc-project-idea-assessing-current-impacts-of-climate-change-in-natural-populations)) as an entry,     
   * or write down notes for a paper([example](https://github.com/adnguyen/Notebooks_and_Protocols/blob/master/2016_notebook.md#id-section36).      

*  Lastly, you can share specific entries because of the three "#" automatically creates a link when the notebook renders on github.      



<a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a> 
.  


### Table of contents for 60 entries (Format is *Page: Date(with year-month-day). Title*)        
* [Page 1: 2017-01-18](#id-section1). First class; intros
* [Page 2: 2017-01-19](#id-section2). Notes from papers discussed in class on 2017-01-23.
* [Page 3: 2017-02-01](#id-section3). Notes on from PuTTY command practice on 2017-02-01
* [Page 4: 2017-02-06](#id-section4). Notes from command line practice 2017-02-06; fastq files
* [Page 5: 2017-02-08](#id-section5). Notes from command practice in class 2017-02-08; Fastqc and winscp
* [Page 6: 2017-02-13](#id-section6). Notes from command practice in class 2017-02-13; bwa aln 
* [Page 7: 2017-02-15](#id-section7). Notes from command practice in class 2017-02-15; SAM file
* [Page 8: 2017-02-22](#id-section8). Notes from command practice in class 2017-02-22; working in R
* [Page 9: 2017-02-27](#id-section9). Notes from command practice in class 2017-02-27; DESeq models 1-3 in R
* [Page 10: 2017-02-28](#id-section10). Notes from command practice HOMEWORK 2017-02-28; DESeq models 4-5 in R
* [Page 11: 2017-03-01](#id-section11). Notes from command practice in class 2017-03-01; WGCNA
* [Page 12: 2017-03-06](#id-section12). Notes from command practice in class 2017-03-06; SNPs
* [Page 13: 2017-03-06](#id-section13). Notes from meeting with Steve 2017-03-06; HW2
* [Page 14: 2017-03-08](#id-section14). Script for Homework #2; 2017-03-08
* [Page 15: 2017-03-08](#id-section15). Notes from command practice in class 2017-03-08; vcftools
* [Page 16: 2017-03-20](#id-section16). Notes from command practice in class 2017-03-20; pop gen
* [Page 17: 2017-03-22](#id-section17). Notes from command practice in class 2017-03-22;  pop gen continued
* [Page 18: 2017-03-27](#id-section18). Notes from class commands 2017-03-27 (Self guided +notes from mtg w/Steve; PCA and DAPC
* [Page 19: 2017-03-29](#id-section19). Notes from class commands 2017-03-29; ADMIXTURE
* [Page 20: 2017-04-03](#id-section20). Notes from class commands 2017-04-03; OutFLANK
* [Page 21: 2017-04-05](#id-section21). Script for Homework 3; vcftools and PCA plots; 2017-04-05
* [Page 22: 2017-04-05](#id-section22). Notes from class commands 2017-04-05; Gene annotation and enrichment 
* [Page 23: 2017-04-10](#id-section23). Notes from class commands 2017-04-10; 16s data analysis (Part 1)
* [Page 24: 2017-04-12](#id-section24). Notes from class commands 2017-04-12; 16s data analysis (Part 2)
* [Page 25: 2017-04-17](#id-section25). Notes from class commands 2017-04-17; 16s data analysis (Part 3)
* [Page 26: 2017-04-19](#id-section26). Notes from class commands 2017-04-19; 16s data analysis (Part 4); PIECRUST
* [Page 27: 2017-05-03](#id-section27). Script for FINAL PROJECT 2017-05-03; DESeq2 and Gene annotation. 
* [Page 28:](#id-section28).
* [Page 29:](#id-section29).
* [Page 30:](#id-section30).
* [Page 31:](#id-section31).
* [Page 32:](#id-section32).
* [Page 33:](#id-section33).
* [Page 34:](#id-section34).
* [Page 35:](#id-section35).
* [Page 36:](#id-section36).
* [Page 37:](#id-section37).
* [Page 38:](#id-section38).
* [Page 39:](#id-section39).
* [Page 40:](#id-section40).
* [Page 41:](#id-section41).
* [Page 42:](#id-section42).
* [Page 43:](#id-section43).
* [Page 44:](#id-section44).
* [Page 45:](#id-section45).
* [Page 46:](#id-section46).
* [Page 47:](#id-section47).
* [Page 48:](#id-section48).
* [Page 49:](#id-section49).
* [Page 50:](#id-section50).
* [Page 51:](#id-section51).
* [Page 52:](#id-section52).
* [Page 53:](#id-section53).
* [Page 54:](#id-section54).
* [Page 55:](#id-section55).
* [Page 56:](#id-section56).
* [Page 57:](#id-section57).
* [Page 58:](#id-section58).
* [Page 59:](#id-section59).
* [Page 60:](#id-section60).   


------

<div id='id-section1'/> 

### Page 1: 2016-07-18. Ecological genomics, first class

### **Steve and Melissa's intro**    
*  Steve: It is a young field, trying to establish it's own identity    
   * Ecological genomics institute, KSU: emphasis on adaptation to environment   
   * Gordon Research Conference: Integrating different levels of biological organization on **ANY SYSTEM**; approach and tool focused! Field going towards new data and new analytic techniques  
   * Intro to eco genomics, oxford press; Using technology to address ecological issues such as nutrient cycling, population structure, life history vairation , trophic interaction, stress responess, and adpatation to environmental change   

*  DATA driven: next gen sequencing revolutionizes biology
   * creats a new problem--large datasets!!! how to make sense? 
   * not data limited and potentially computationally limited   

*  Where is the field headed    
   * Molecular Ecology Journal(flagship journal representative o the field)  
      * ALL systems:  corals, protists, daphnia, coral, lemurs, dandelions, steve studies trees 
      * model organism constraint disappearing!   
   * What types of questions are asked?  
      * How do genes correspond with circadian rythm?  
      * How does the microbiome influence the organism? 
      * How does epigenetic variation influence evolutionary responses? or contribute to phenotypic variation?  
      * What are the patterns of genetic diversity that can give us insights on population dynamics?  
      * What are constraints and tradeoffs and genetic mechanisms of traits? 

*  Methods?   
   * De novo genome assembly; sequencing a DNA book from scratch!!    
     * RNA-seq; transcriptomic profiling     
   * 16 s metagenomic sequencing      
   * Rad-seq/GBS for estiamting population structure and genetic diversity     

*  Proccesses studied?    
   * All evo and eco stuff; speciation, hybridization, local adaptation, genetic basis of local adaptation, genetic architecture of complex phenotypes, genes controlling host-pathogen evolutionary dynamics, pop structure, gene flow, epigenetics     

*  Goals of the course!    
   1. Learn how ecology and genomes shape each other   
   2. Think creatively about major questions, and pose testable hypotheses to those questions using appropriate genomic data    
   3. Think about careful experimental design and statistical analysis---shown by reading papers   
   4. Achive working knowledge and level of comfort for bioinformatics routines for ecological genomics studies   
      `'
   ### Melissa background  

**Background, what drove Melissa and Steve to ecological genomics?**       

Melissa read a cool paper that scales from analyzing a few loci to the whole genome.   

One figure popped out at her, FST (developed by Sewell Wright) histogram.   FST of 1= complete differentiation, FST of 0 = no diff. FST described as **Alleles in space**. From this histogram, Melissa was struck by how you can separate out neutral from selective ones.  

Melissa has a data set with 96 sea stars and then the 16s microbiome. Would be cool to see if there is heritability in some bactera
'
### Steve background   

* Inspired by Yanis Antonivics (an **OG**)   
* At the time, just so stories: **Adaptatationist programme**    
  * Just go out and go by feeling in a natural history way and prescribe an adaptation story   

* Janis wrote a creed to quantify the operational relationship between traits, environment, and genetics     
  * Yanis was on Steve's committee and Steve was interested in adaptation with respect to invasion biology because organisms need to respond to novel environments     * Phenotypes can relate to the environment, but what is the genetic basis of local adaptation (in situ)? There are other confounding issues: demographic effects, plasticity     
  * Steve thinks about environment-phenotype-genetics triangle. Basically a path diagram that feeds back on each other.    * Relationship between genes and phenotype ---GWAS (Genome wide association study)    
  * Relationship between genetics and environment --- Fst, clines between allele frequencies and environment    
* Invasion history is tough because of demographic history    
* He decided to focus on trees; large population size, straddle huge environmental gradients so the opportunity for selection is high   
  * positive relationship between Growing season length and traits    
  * Did a  reciprocal transplant of different populations to identify the extent of local adaptation in large established common gardens    
  * SK does GBS (genotype by sequencing)      
  * Problem with field: validating key gene candidates       


------

<div id='id-section2'/> 

### Page 2: 2017-01-19. Notes from papers discussed in class on 2017-01-23.
------

<div id='id-section3'/> 

### Page 3: 2017-02-01. Notes on from PuTTY command practice on 2017-02-01.   
I am going to make a new directory called scripts:   

    cd ~/     
    mkdir scripts   
    ll   

I opened the ~/ directory and it shows both mydata and scripts directories; good!   

    cd scripts    
    ll   

When I open scripts it showed nothing inside...totally as it should be   

Now I am going to move trim_example.sh to my scripts directory   

    cd ~/mydata   
    mv trim_example.sh scripts   
    cd ..   
    cd scripts   
    ll   

It wasn't there so I'm going to try again   

    mv trim_example.sh ~/scripts   

It said "no such file"; now there is no "trim_example" but there is a file called "scripts" so I might have renamed it?   

    head scripts   

Yup I accedentally renamed it; so I'll name it back to "trim_example" and then move it   

    mv scripts trim_example.sh   

Succesfully renamed! NOW I'm going to move it to scripts   

    mv trim_example.sh ~/scripts   
    cd ..   
    cd scripts   
    ll   

Its there! phew haha.  Now i'm going to move ssw_samples.txt from the server /data/project_data/ folder to my personal directory (~/mydata)  

    cd /data   
    cd project_data/   
     cp ssw_samples.txt  ~/mydata   
    cd ~/mydata   
    ll   

Its there!   From the ssw_samples.txt file I will create a HHonly and SSonly file.  First the HHonly:

    grep 'HH' ssw_samples.txt   
    grep 'HH' ssw_samples.txt >ssw_HHonly.txt   
    ll   
    head ssw_HHonly.txt   

Success!  Now the SSonly file:   

    grep 'SS' ssw_samples.txt >ssw_SSonly.txt   
    ll   
    head ssw_SSonly   

Success again!  Now we are going to move these two files to a new directory which we first need to make:

    mkdir sample_by_disease   

Great! now to move the files:

    mv *only.txt sample_by_disease/   
    ll   
    cd sample_by_disease/   
    ll   

It worked!   That will be all for today. =) 

------

<div id='id-section4'/> 

### Page 4: Notes from command line practice 2017-02-06; fastq files  
First thing: we will look at the fastq files in the server directory:   
```   
cd /data   
ll   
cd project_data/   
ll   
cd fastq   
ll   
```
gz=zipped    
file name = ```07_5-08_S_1_R1```
* 07 = individual   
* 5-08 = date   
* S = healthy/sick (sick)   
* 1 = degree of health from 0-5   
* R1 and R2 indicate left and right read (since it was paired reads)   

I will work on files:    
* ```15_5-17_S_3_R1.fq.gz```   
* ```15_5-17_S_3_R2.fq.gz```   

Unzip them with "zcat"  (ex: zcat FILENAME | head)   
```   
zcat 15_5-17_S_3_R1.fq.gz | head   
zcat 15_5-17_S_3_R2.fq.gz | head   
```
This is what it looks like when you unzip:   
```   
@J00160:  
63:  
HHHT2BBXX:1:1101:  
26839:  
1261 2:N:0:TCCGGAGA+AGGATAGG
CGGCTACCACATCCAAGGAAGGCAGCAGGCGCGCAAATTACCCACTCCCGACACGGGGAGGTAGTGACGAAAAATAGCAATACAGGACTCTTTCGAGTTCC
+
AAFFFJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJ<JJJJJJJJJJJJJJJJJJJJJJJJFJJJJFJJAJJJJFJJJJJJJJJJJJJJJJJJJJFJFJAF
```
__1st line__: info on sequencer etc   
__2nd line__: actual sequence (should be 100base long)   
__3rd line__: +   
__4th line__: Quality scores for each base (link in tutorial will tell you what each means; J and K are good)   
* a quality score better then or equal to 30 is typically used; therefore you want to see letters in your quality score.      

Use program called "FastQC" to look at quality more systematically   
```   
cd /data/scripts/   
ll   
cp trim_example.sh ~/mydata/   
```
People put it in their scripts that they created last week but my computer was dead so I don't have one thats why I put it in "mydata" *see page 3 for updated location*   
```   
cd ~/   
ll   
cd mydata   
ll   
```
Its in there!  We were just checking if they file copied over   

**NOTE**: control C cancelles a command which is running   
```   
head trim_example.sh   
```
The above command shows you the first ten lines of that file   
```   
vim trim_example.sh   
i     
```
"i" allows you to start editing   

We are doing the trimomatic program then we will do data visulization   

Everywhere that there is the word "samp" or "yoursample" we should put in our sample name   

change path from:   
```   
 				 /data/project_data/rawreads/filtered/YourSample_R1.fq.gz \
                 /data/project_data/rawreads/filtered/YourSample_R2.fq.gz \
                 ~/cleanreads/"samp_R1_clean_paired.fa" \
                 ~/cleanreads/"samp_R1_clean_unpaired.fa" \
                 ~/cleanreads/"samp_R2_clean_paired.fa" \
                 ~/cleanreads/"samp_R2_clean_unpaired.fa" \
```
to   
```   
 				/data/project_data/fastq/YourSample_R1.fastq \
                 /data/project_data/fastq/YourSample_R2.fastq \
                 ~/cleanreads/"samp_R1_clean_paired.fa" \
                 ~/cleanreads/"samp_R1_clean_unpaired.fa" \
                 ~/cleanreads/"samp_R2_clean_paired.fa" \
                 ~/cleanreads/"samp_R2_clean_unpaired.fa" \
```
copy = CTL C    
paste = CTL V   
When done with edits it should look like:   
```   
				/data/project_data/fastq/15_5-17_S_3_R1.fq.gz \
                 /data/project_data/fastq/15_5-17_S_3_R2.fq.gz \
                 /data/project_data/fastq/cleanreads/"15_5-17_S_3_R1_clean_paired.fa" \
                 /data/project_data/fastq/cleanreads/"15_5-17_S_3_R1_clean_unpaired.fa" \
                 /data/project_data/fastq/cleanreads/"15_5-17_S_3_R2_clean_paired.fa" \
                 /data/project_data/fastq/cleanreads/"15_5-17_S_3_R2_clean_unpaired.fa" \
                 ILLUMINACLIP:/data/popgen/Trimmomatic-0.33/adapters/TruSeq3-PE.fa:2:30:  
10 \
                 LEADING:28 \
             TRAILING:28 \
             SLIDINGWINDOW:6:28 \
             HEADCROP:9 \
             MINLEN:35 \
```
We should have changed:   
* file names (samp/yoursample to 15_5-17_S_3_R1)   
* path (2 paths for input; 4 paths for output)   

To save:   
* Press esc   
* ":w" write   
* ":q" quit   
```   
:w    
:q    
```
You can combine commands with:   
```   
:wq   
```
 Now you can make your script exicutable; if it is already exicutable it will be green   

You can run your program by:   
```  
./trim_example.sh    
```
It will tell you "completed successfully" when done; will tell you % that "survived" etc    

What it looks like when you run:   
```   

TrimmomaticPE: Started with arguments: -threads 1 -phred33 /data/project_data/fastq/15_5-17_S_3_R1.fq.gz /data/project_data/fastq/15_5-17_S_3_R2.fq.gz /data/project_data/fastq/cleanreads/15_5-17_S_3_R1_clean_paired.fa /data/project_data/fastq/cleanreads/15_5-17_S_3_R1_clean_unpaired.fa /data/project_data/fastq/cleanreads/15_5-17_S_3_R2_clean_paired.fa /data/project_data/fastq/cleanreads/15_5-17_S_3_R2_clean_unpaired.fa ILLUMINACLIP:/data/popgen/Trimmomatic-0.33/adapters/TruSeq3-PE.fa:2:30:  
10 LEADING:28 TRAILING:28 SLIDINGWINDOW:6:28 HEADCROP:9 MINLEN:35
Using PrefixPair: 'TACACTCTTTCCCTACACGACGCTCTTCCGATCT' and 'GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT'
ILLUMINACLIP: Using 1 prefix pairs, 0 forward/reverse sequences, 0 forward only sequences, 0 reverse only sequences
Input Read Pairs: 2016797 Both Surviving: 1721399 (85.35%) Forward Only Surviving: 216190 (10.72%) Reverse Only Surviving: 35588 (1.76%) Dropped: 43620 (2.16%)
```
* **ILLUMINACLIP** = its finding the adaptors and triming them    
* **SLIDINGWINDOW** = 6:28 = across 6 nucleotides...   

The link to the program website will tell you what each command within the "trim_example" file means  
```   
fastqc 15_5-17_S_3_R1.fq.gz   
```
Says that this file dosen't exist.  is that because i'm giving the command from the wrong folder   

I tried again in the "/data/project_data/fastq/" place and it worked.   

Now to move our html file to our computer:
```   
scp hlachanc@pbio381.uvm.edu:/data/project_data/fastq/15_5-17_S_3_R2.fastqc.html .   
```
Says no such file/directory; I suppose I'll try again later? It looks like the file is there

I decided to try running this command while I was in the directory with the fastq files (/data/project_data/fastq):   
```   
scp hlachanc@pbio381.uvm.edu:/data/project_data/fastq/15_5-17_S_3_R1_fastqc.html .
```
IT worked! =D   
Now I just need to figure out how to open it...   

In the mean time I decided to run the Fasqc for the R2 of my sequence:   
```   
fastqc 15_5-17_S_3_R2.fq.gz   
```
It ran succesfully however I'm not going to copy the html until I understand why we are doing that





------

<div id='id-section5'/> 

### Page 5: Notes from command practice in class 2017-02-08; Fastqc and winscp

```   
cd /data/project_data/fastq/cleanreads   
```
Now I need to run fastqc on each of the 4 files trimomatic generated and put into "cleanread" directory   
```   
fastqc  15_5-17_S_3_R1_clean_paired   
```
Didn't work. tried this:   
```   
fastqc 15_5-17_S_3_R1.fq.gz_clean_paired   
```
Didn't work   
```   
fastqc 15_5-17_S_3_R1_clean_paired.fa   
```
This worked   

Run FASTqc for the other three   
```   
fastqc 15_5-17_S_3_R1_clean_unpaired.fa   
```
Worked   
```   
fastqc 15_5-17_S_3_R2_clean_paired.fa   
```
Worked   
```   
fastqc 15_5-17_S_3_R2_clean_unpaired.fa   
```
Worked   

Go to winscp and move the 4 html files from cleanreads to your desktop and then open them to view the report   

In the report you will see green yellow and red icons off to the side.  They mean its good iffy and warning it might be bad   

Now we will use trinity to do some assemblies: start with two assemblies to compare   

You usually want the "paired" file that trimomatic generated when doing assemblies   

We will do an "experiment" to see how the transcriptom assembly can varry between individuals, H vs S etc   

Experiment we choose was to look at H vs S in the same individual (individual 19, H vs S)   
```    
Trinity --seqType fq --left(r1 paired) reads_1.fq --right(r2 paired) reads_2.fq --CPU 6 --max_memory 20G    
```
We will run this next week   

Screen is a command you can run that will let you run commands while you aren't logged on; good for commands that take a while.    

In trinity there is trinity stats that let you see #genes, contig legths, etc   

We will continue with mapping reads on Monday   

------

<div id='id-section6'/> 

### Page 6: Notes from command practice in class 2017-02-13; bwa aln 

The general work flow:   

1. Clean and evaluate reads (.fastq)  
   - we used trimmomatic    
2. Make and evaluate a transcriptome assembly (.fasta)  
   - we were going to do an experiment but ran out of time so looked at cleaned assembly (run with Trinity); results at the bottom of the tutorial for 2-6-17 (add link)   
3. Map cleaned reads to the transcriptome assembly (makes .sam files)   

From these sequence alignment files, we can extract two types of information:    

- (a) read counts - the number of reads that uniqely map to each “gene”   
- (b) single nucleotide polymorphisms between a sample and the reference
  ​    

With these two types of data, we can go on to differential gene expression analyses and population genomics.   

"Open reading frames": allow us to say We want complete transcripts and we want the longest one   

- She already did **Transdecoder** (only needs to be done once so we don't need to run it); how she ran it is on the tutorial (add link to tutorial here)   
- She started a **blast run** (which is probably done now)

Options for improving this assembly include:   
(1) using more reads from other individuals or trying a different individual   
(2) changing the cleaning and assembly parameters. We can evaluate based on the percentage of genes that have good blastp hits and the percentage of single copy orthologs included in the reference (for example using the new program BUSCO.)   

- She created **reference transcriptome** which we will compare ours too (will save us time)   
  - This is common; pick an individual to create a reference transcriptome and then compare the rest to it   
  - She used individual 15 for the reference transcriptome (complied all the time points etc to make one transcriptome for that individual)  
- Trinity.fasta = the results from that individual

```
#!/bin/bash/
cd /data/popgen/databases/
makeblastdb -in uniprot_sprot.pep -dbtype prot -out uniprot_sprot
blastp -query /data/project_data/assembly/Trinity.fasta.transdecoder_dir/longest_orfs.pep  -db /data/popgen/databases/uniprot_sprot  -max_target_seqs 1 -outfmt 6 -evalue 1e-5 -num_threads 10 > blastp.outfmt6
```

"makeblastdb" sets up the data base you will blast to so you can blast in here   
"These transcriptome assembly processes are ongoing. But for now we will map to the 5,693 “genes” based on the longest ORFs"   

**NOW** I will start doing commands:   

```
cd /data/scripts/
ll
cp bwaaln.sh ~/scripts
cd ~/scripts
ll
```

Its in there!  We just copied the file bwaaln.sh to my personal scripts folder   

Next we will edit the file and insert our left read file name:   

```
vim bwaaln.sh
i 
```

go to "my left" and switched it with 15_5-17_S_3_   

NOTE: she renamed our files   

```
myRight=${15_5-17_S_3_R1.fq.gz_left/_R2.fq.gz_right}
```

This file (bwaaln.sh) tells it that my left = my right   
echo is a program that prints to screen   

```
myShort=`echo $myLeft | cut -c1-11`
```

This command says to print "my left" to this cut function and tell it to cut the first 11 digits   

Indexing step (only needs to happen once; already done)   

use this bwa aln program   
Opened new putty window and ran:   

```
bwa aln
```

It gives you a list of all the areas you can control; we used defult perameters   

it will run the bwa aln on your left...:   

```
bwa aln /data/project_data/assembly/longest_orfs.cds /data/project_data/fastq/cleanreads/$myLeft > $myLeft".sai"
```

...And your right files:   

```
bwa aln /data/project_data/assembly/longest_orfs.cds /data/project_data/fastq/cleanreads/$myRight > $myRight".sai"
```

and generate a few file (ending in ".sai")   
Then it will run the last command which will create a combined aligned file:   

```
bwa sampe -r '@RG\tID:'"$myShort"'\tSM:'"$myShort"'\tPL:Illumina' \
        -P /data/project_data/assembly/longest_orfs.cds $myLeft".sai" $myRight".sai" \
        /data/project_data/fastq/cleanreads/$myLeft \
        /data/project_data/fastq/cleanreads/$myRight > $myShort"_bwaaln.sam"
```

We go from two cleaned files -> two intermediate files (.sai) to one combined file (.sam)   

To run this file:   

```
bash bwaaln.sh >> bwaoutput.txt
```

After it ran I opened the text file to see what was in there; it was just the file names that the file told to "echo" or print to screen   

My .sam file saves to the directory I ran the script in (~/scripts)   

```
cd ~/scripts/
ll
```

It is there!   
**BUUUT** it was wrong so I need to run the script again:   

I went back into the bwaaln.sh file and changed THIS:   

```
myLeft='15_5-17_S_3_R1.fq.gz_left_clean_paired.fq'
```

Instead the my left here:   

```
myRight=${myleft/_R1.fq.gz_left/_R2.fq.gz_right}
```

Once I edited the file I ran it again:   

```
bash bwaaln.sh
```

It ran   

```
ll
```

The new file has the right name! =D   

**HELPFUL TRICK**:     

To have it continue to run a command and close your screen:      

```
screen
```

To resume the screen that you detached from:   

```
screen -r
```



------

<div id='id-section7'/> 

### Page 7: Notes from command practice in class 2017-02-15; SAM file

**Coding outline:**

1. lab notebook (how to put notes into git hub)
2. SAM files
   1. Extract expression
3. What's going on in the background

**No class on Monday 2/20**

**Lab Notebook:**

1. Need: Typora and notebook.md (in github)
2. open temp in Typora
   - edit (use md cheat sheet)
   - save
3. Go to github desktop repository (repo)
   - commit to master
   - sync
4. Check that it is updated online



*Andrew's office hours:* **Friday 3-5pm**



*NOW* we will start coding

Go to your SAM file:

```
cd scripts
ll
```

SAM file is in there!

Next, save the last 100 lines:

```
tail -n 100 15_5-17_S_3_bwaaln.sam > tail.sam
vim tail.sam
:set nowrap
```

For some reason my SAM file ***didn't run all the way*** so I opened Melissa's file so I could follow along

```
cd /data/project_data
cd fastq
cd cleanreads/test
vim tail.sam
```

It should look like this:

```
J00160:  
63:  
HHHT2BBXX:4:2228:  
7953:  
48931   77      *       0       0       *       
J00160:  
63:  
HHHT2BBXX:4:2228:  
17655:  
48931  77      *       0       0       *       
```

The "77' (second column) is your SAM FLAG which you can check what it means by inserting the # into the flag decoder provided on the tutorial

Here is a list of what each column means in the SAM file:

- **1st column:** the read, aka. query, name,
- **2nd column:** a FLAG (number with information about mapping success and orientation and whether the read is the left or right read),
- **3rd column:** the reference sequence name to which the read mapped
- **4th column:** the leftmost position in the reference where the read mapped
- **5th Column:** the mapping quality (Phred-scaled)
- **6th column:** a CIGAR string that gives alignment information (how many bases Match (M), where there’s an Insertion (I) or Deletion (D))
- **7th column:** an ‘=’, mate position, inferred insert size (columns 7,8,9)
- **8th column:** the query sequence and Phred-scaled quality from the FASTQ file (columns 10 and 11)
- **9th column:** then Lots of good information in TAGS at the end, ***if the read mapped***, including whether it is a unique read (XT:A:U), the number of best hits (X0:  
  i:1), the number of suboptimal hits (X1:  
  i:0).

example:

```
 RG:Z:38_6-24_S_5        XT:A:U  NM:i:0  SM:i:37 AM:i:0  X0:  
i:1  X1:  
i:0  XM:i:0  XO:i:0  XG:i:0  MD:Z:89
```

The BWA man page (link on tutorial) shows you what each TAG means

Use the grep command to tell you how many reads mapped uniquely

```
grep -c XT:A:U 38_6-24_S_5_bwaaln.sam
```

I got 1177827   

above looks at XT which is unique (see BWA man page for more detail)

also try this one:

```
grep -c X0:  
i:1 38_6-24_S_5_bwaaln.sam
```

I got 1182952   

The first grep command seemed to be the better indicator of the # of uniquely mapped reads

Go and find the python script:

```
cd /data/scripts
ll
```

Copy the python script to where your sam file is:

```
cd /data/scripts
cp countxpression_pe.py ~/scripts
python countxpression_pe.py 20 35 countstatssummary.txt YOURFILENAME.sam
```

This python script will take the sam file and make a list of the genes and counts how many reads uniquly mapped to each gene 

We are running:

```
sed -i 's/::/\_/g' yourfile.sam 
```

This lets you change something within your file:

​	s = search
​	:: = find this
​	_ = replace with this
​	g = globally 

This python can take a while so run in 'screen'

When you open the count file we are interested in the first column

- **1st column** # of unique reads


- **2nd column** # reads that mapped to this gene *and* other genes
- **3rd column** # reads total (unique + mapped to other genes)

2 files generated from python: countoutout; countstatssummary

*only 7.4% were quality aligned (thats why we need a better assmebly which will be done by Melissa before next class)





------

<div id='id-section8'/> 

### Page 8: Notes from command practice in class 2017-02-22; working in R

#### **Make sure you have:**

- R
- R studio
- DESeq downloaded in R studio (copy commands from bottom of tutorial from last class)
- move files from /data/project_data/DGE to your computer (use WinSCP)
  *(I saved them to a folder on desktop called "class 2-22 files for R")
  -open "DESeq2_exploreSSW_trim.R" in R studio  (simply go to: file -> open file -> then open this file)

#### **Other Notes:**

- Melissa made a new transcriptome assembly (since our previous mapping was not great)
- Melissa made assembly on server (hard because it has limited memory)
- these are reads that are mapping to the assembly
- final assembly 26000 genes
  D- countdata_trim.txt has only some of the samples (allcounts data file not up yet but it wil have all samples)

### **Working in R**

- how to run code (2 ways):
- have your cursor in the line you want to run then hit "CTL R"
- highlight and run



Thus far (lines 12-41) we have looked at data that we already had; we havent generated any new data yet

- first data we generate is when we run "dds <- DESeq(dds)..."



Overall our assembly is still not great (better but still lots of low counts)



**plotMA**

- Above: highly expressed and healthy
- Below: highly expressed and sick



**Interaction maps:** (around lines 145)

- 1 gene

- healthy and sick; intertidal and subtidal

- y axis: normalized read counts 

  ​

**Issues with line 97 (scale for y...)



Melissa will try to improve mapping again by Monday (so we can BLAST)

------

<div id='id-section9'/> 

### Page 9: Notes from command practice in class 2017-02-27; DESeq models 1-3 in R

## Coding session:

1) move new data 
​	moved all the data from DGE to my desktop by using winSCP to drag and drop
​		stored them in folder on desktop called "DGE data from 2-27" 
2) DESeq2 (5 models)
3) your models



## Begin coding

open R (open script in R or double click file to open)   

*Andrew suggested randomly sampling the data; we will sample 10% of data using random sampling function     


***1st homework:*** setting up differently expression analysis; this will be practice for that



### Model #1 (TEST EFFECT OF HEALTH CONTROLLING FOR LOCATION); Lines 18-81

```
Ran commands from line 1-13 all at once
```


**Line 1:** change this to be a directory on your computer

**Lines 2 and 4** loads programs

**Lines 6-8** preparing and checking data to be used in R

**Line 6:** opened the new "contsdata_trim2" file
​	*header* = true; first line is header
​	*strings as factors*; letters = factors not numbers
​	*row.names* = 1 there are names in this row 

**Line 7:** load meta data 
​	*as.matrix* stores it as an r object converst into data matrix form

**Line 8:** look at head of data to check


**Line 18:** dds <- DESeqDataSetFromMatrix(countData = countData, colData = colData, design = ~ location + health)    
​	it wants: count daa, col data, and design   
​	we created count data and col data files earlier (lines 1-13)  
​	*Desgin is important; here we are looking at **health** as the MAIN EFFECT   
​	this will test for an effect of the last term while controling for an effect of the other term(s) listed  
​	We are: testing health while controling for differences in location  
​	trying to remove variance of location and then testing variance of health   
​	*if you wanted to test for health in general you can delete location and only type in "health"  
​	trying to remove variance of location and then testing variance of health   

***NOTE:*** all plot commands are at the bottom; at any point you can look at a plot of your data but you will need to change the conditions  

```
Run line 18 then line 26
```

shows 13053 (number of genes in transcritome assembly that we mapped to) 77 (number of samples)



**Line 29:** says it needs at least 100 reads to make it valid

```
Run lines 29 and 30
```

It shows: 12954 (number of genes in transcritome assembly that we mapped to) 77 (number of samples)  
​	Means that about 100 genes (13053-12954) that didn't have any reads  
​	only losing 100 is much better then our last class assembly   

**Line 35:** this randomly chooses 1200 rows from our 12000 so we can have a smaller set to work with when running the models next  
​	***NOTE:*** THIS IS FOR **PRACTICE ONLY** normally you would want to run all

**Line 38: ** by putitng "H" first we are saying the healthy are the set to which we should compare

```
Run lines 35, 36, 38 and 40
```

**Line 43:** sorts by p value so you can look at top 6 significant genes

**Line 45:** "# log2 fold change (MAP): health S vs H"  since the S came first it is the "up" one in summary results

```
run lines 42, 43, 44
```

**Line 73:** does a summary of what we just ran (res; aka resutls)
shows that LFC going up (sick) has 24 genes that were more highly expresssed in sick vs healthy; and LCF (log fold change) going down (healthy) has 8 



### Model #2 (interactions); Lines 86-142

*Basically this is very simliar to model #1 but we added the new interaction and we are using more cores to process the data (parallel)

**line 86:** same as line 18 except design has a new interaction added "location:health"

**line 97:** "parallel" uses two cores of your computer to run the script (goes faster)

```
run lines 86-100
```

gives you "# [1]  "Intercept"           "location_sub_vs_int" "health_S_vs_H"       "locationsub.healthS""  

* just shows locationsub interacting with healthyS but her understanding is it is running all versions of interactions  
* only showing results names at this point   



***NOTE:*** if you ever have a question about a command you can seach the DEseq tutorial or type "?command"



```
Run lines 103,104,105,134
```

Shows that the more factors you use you might miss picking up certain data; therefore do lots of models and compare

**To save results:** command not in this script but you can look up in tutorial; might also be in 1st version of script

can have it show you only significant; "res <- res[order(res$padj> 0.05),]" (something like that; she will update to include



### Model #3 (GROUP DESIGNS can be used for contrasts of interest or interactions); lines 147-198

**Line 147:** set up what groups you want (location, health)

**Line 148:** desing is "group" lumps all possible groups (for variables that you specificed in the line above) into a group

**Line 159:** shows you the groups

**Line 163:** sets up what groups you want to contrast; here we contrast inter vs sub at various healthy and sick levels

**Benefit of group; lets you focus on specifc variables (inter vs sub, day, etc)

------

<div id='id-section10'/> 

### Page 10: Notes from command practice HOMEWORK 2017-02-28; DESeq models 4-5 in R  
**HOMEWORK:** Finish running through the DESeq2 script by 3-1-17
We had completed running model 3 therefore I have to do:

- model 4 (lines 204- 215)
- model 5 (lines 220-260)
- any plots at the bottom of the script

### Model 4 (TIME SERIES USING REDUCED MODELS); lines 204- 215

Start by loading the programs again (run lines 2 and 4)

Looks similar to the other models except:
​	**Line 204:** the command is "ddsTS" instead of "dds" and there is no "design" section.  command below for comparison

```
Model 4: ddsTS <- DESeqDataSetFromMatrix(countData = countData, colData = colData, ~ health + day + health:day)
Model 3: dds <- DESeqDataSetFromMatrix(countData = countData, colData = colData, design = ~ group)
Model 1: dds <- DESeqDataSetFromMatrix(countData = countData, colData = colData, design = ~ location + health)
```

Now I will try running line 204:  


```
Run line 204
```

It said, "Error 'countData' not found"
I went back and ran lines 6-13
This time it worked

**Lines 205, 207, and 208:** similar to lines 29, 35 and 38

```
run line 205 and 207 and 208
```

I link model 4 is really just looking at health vs day and the key spots you see changes to the commands are:

- line 204 (end of command)


- line 210 ("reduced = ~ health + day")

```
Run lines 210-215
```

Summary showed:

```
out of 1198 with nonzero total read count
adjusted p-value < 0.1
LFC > 0 (up)     : 0, 0% 
LFC < 0 (down)   : 0, 0% 
outliers [1]     : 16, 1.3% 
low counts [2]   : 2, 0.17% 
(mean count < 0)
[1] see 'cooksCutoff' argument of ?results
[2] see 'independentFiltering' argument of ?results
```

Is that correct?

### Model 5 (EFFECT OF DISEASE SEVERITY SCORE); Lines 220-260

Similar to other models but the desgin this time is "~ score"

```
run lines 220-262
```

After trying to run line 220 and got this message


```
the design formula contains a numeric variable with integer values,
  specifying a model with increasing fold change for higher values.
  did you mean for this to be a factor? if so, first convert
  this variable to a factor using the factor() function
```

Continued running the other commands anyways and ended with this:

```
out of 1196 with nonzero total read count
adjusted p-value < 0.1
LFC > 0 (up)     : 22, 1.8% 
LFC < 0 (down)   : 4, 0.33% 
outliers [1]     : 7, 0.59% 
low counts [2]   : 508, 42% 
(mean count < 12)
[1] see 'cooksCutoff' argument of ?results
[2] see 'independentFiltering' argument of ?results
```

seems similar to what she got in the script example.

Take aways:
​	I can run these models but I'm not sure what exactly they just did. (will ask quesstions in class)



Tried running a few **plots** but don't know enough about them to troubleshoot yet.  Will ask for help/try again later.   

------

<div id='id-section11'/> 

### Page 11: Notes from command practice in class 2017-03-01; WGCNA  

WGCNA_tutorial_SSW.R (DONE IN R)

Run lines 1 - 5

If you want you can go to link on line 10 to download data
*We will be using our data

Run line 20 (shows you what directory you are getting data from)
	change if needed; i'm already in the right one

### Chunk 1
``` 
Run lines 31, 39, 41 , 42
```

### Chunk 2
```
sdatExpr0 = as.data.frame(t(starData[, -c(1:1)]));
dim(sdatExpr0)
names(sdatExpr0) = starData$X;
rownames(sdatExpr0) = names(starData)[-c(1:1)];
```

-c taking out the first row and column so it won't include row and column names
 *anything with "s' in front is sea star data commands

```
Run lines 57-60
```

### Chunk 3

```
Run lines 73, 74
```
According to the help function in R the "goodSamplesGenes" command:
"...checks data for missing entries and zero-variance genes, and returns a list of samples and genes that pass criteria maximum number of missing values. If necessary, the filtering is iterated."

Therefore I'm assuming this command is pre-checking the quality of the genes before the nex step?
The cut off of "good genes" we used seems to be 3...? (not sure what 3 means or what it was choosen)


### Chunk 4

```
Run 96-105
```

This chuck sounds like it is removing the genes that did not met the criteria in chunk 3.
You can also have it print (aka tell you) the names of the ones removed
Don't know word for word what the command lines do but that is the genral idea

**Added an 's' to line 101 so that 'gsg' becomes 'sgsg'


### Chunk 5

```
Run lines 125 -133
```

'sizeGrWindow' allows you to set the size of the window for the graph you are creating
we are creating a 'sampleTree' which accocrding to R help is not a command in its self:
R suggested the command might be'sampleNames,AnnotatedDataFrame-method' which according to the help function is:
An AnnotatedDataFrame consists of two parts. There is a collection of samples and the values of variables measured on those samples. There is also a description of each variable measured. The components of an AnnotatedDataFrame can be accessed with pData and varMetadata.

Showed some graph thing after running chunk 5

### Chunk 6

```
Run lines 144, 146, 147, 149-152
```

I see a red line was added to the graph but Idk what else happened

### Chunk 7 

We need to open something (Txt file? countsdata_trim?) and remove the outlier samples I10_5.14_H_0

don't need to do removed columns step

------

<div id='id-section12'/> 

### Page 12: Notes from command practice in class 2017-03-06;SNPs  

Over the weekend Steve and Melissa called SNPs with two methods:

- samtools
- reads2snp

But they ran all 94 samples instead of focusing on the 24 individuals so they will re run
BUT we need to choose which method to use to run (See below for options)

Two ways we can approach this data
1) merge all data from each individual so we are only focusing on individuals
* samtools has a merge function in it
* (+) to this method: increase depth for each individual (increas # of reads)

2) take each replicate per individual and call SNPs seperate then compare reps within individuals 
* alread set up to run #2
* less depth but can compare replicates within each individual
* **We decided to run this method

for the reads2snp he took the earliest date from each individual so we have a sub samples to work with today

Core info we are after:

**Position:** Where is the SNP located within a contig or chromosome of the reference assembly?
**Alleles:** What are the alleles present at a given SNP? Are there only 2, or are there more? Are they single-nucleotide differences?
**Depth:** How many reads cover a given SNP? How many reads were observed for each allele?
**Genotype Quality (GQ):**  How confident are we that we're calling the correct genotype (ex., AA, AT, or TT)?
**Sample Names:** Where are the data for each individual sample?

Type of file that gives us the above info is called a VCF (variant call format) file
* you have a header with:
* trascript ID
* ID(not for us)
* REf (allele present in the assmebly)
* ALT (whatever other alleles it found)
* QUAL (how confident it is that there is a snp there)
  	Info (lots of different kinds of info; ie whats its depth (dp) genotype quality, evidence of parology)
  Header is followed by data:
  	Example: G    A 
  	0/0 = GG
  	1/0 = GA
  	11/ AA

VCFtools:
	Should help us filter snps based on depth, genotype quality, etc
	calc statistics: deviation from HWE, pie etc
	allows you to subset data file into certain SNPs you are interessted in.

COMMANDS IN PUTTY:

cd to reads2snps
vim head_SSW_bamlist.txt.vcf
:set nowrap

*"unres" = unresolved (ie seen with 08_5-08_H_0)
applied two filters:
Minimum depth to call a genotype = 10 reads
Minimum genotype posterior probability = 0.95

If it didn't meet these filters it will list is as missing data(.|.)(.)

run:
vcftools --vcf SSW_bamlist.txt.vcf

run: grep "unres" SSW_bamlist.txt.vcf | wc
wc: word count
This is counting every time the word "unres" is seen

1028494934

Now run grep "para" SSW_bamlist.txt.vcf | wc

  4354  143652  795592

Therefore we have ___ we can remove

Initial    7.47M
unres:     5.63 M
PARAlogy:   4354
Remainder: 1.8 M SNPs 

Now we only want biallelic sites so we will run:
```
reads2snps]$ vcftools --vcf  SSW_bamlist.txt.vcf --min-alleles 2 --max-alleles 2
```
aftre running it says
```
After filtering, kept 20319 out of a possible 7472775 Sites
```

brings us way down

MAF: minor allele frequency
run MAF at 0.02
```
vcftools --vcf  SSW_bamlist.txt.vcf --maf 0.02
```

got
```
After filtering, kept 5656584 out of a possible 7472775 Sites
```

Still kept alot (because...)


A common cut of is allow 20% missing data

run:
```
vcftools --vcf  SSW_bamlist.txt.vcf --max-missing 0.8
```

you get:
```
After filtering, kept 100219 out of a possible 7472775 Sites

```

Normally you won't run each filter one at a time so you would combine your filters into one command:

```
vcftools --vcf SSW_bamlist.txt.vcf --min-alleles 2 --max-alleles 2 --maf 0.02 --max-missing 0.8 --recode --out ~/biallelic.MAF0.02.Miss0.8
```

The new part to the above command is "--out ~/biallelic.MAF0.02.Miss0.8"
that is just the file you want it to save to

after it ran is gave us:

```
After filtering, kept 1180 out of a possible 7472775 Sites
```

This means we only have about 1180 SNPs which is low but we did only run 1 from each individual

Go check on file by going to your home directory (~/) and opening the head of the file you just created
```
vim biallelic.MAF0.02.Miss0.8.recode.vcf
:set nowrap
```

Now we will run the hardy winberg test on our data:

```
vcftools --vcf biallelic.MAF0.02.Miss0.8.recode.vcf --hardy
```

We can look at the head of the file it generated:

```
head out.hwe
```

looked wierd so we are reading it with R WITHIN the command line:
```
R
```
opens R 
```
hardy <- read.table("out.hwe", header = T)
str(hardy)
```

```
hardy[which(hardy$P_HET_EXCESS<0.001),]
```

output just the rows that have heterozygous excess when p<0.001

```
hardy[which(hardy$P_HET_DEFICIT<0.001),]
```
ouput just the rows that have heterozygous deficit when p<0.001

Looks like this:
```
                                                                 CHR POS
291 TRINITY_DN45155_c27_g1_TRINITY_DN45155_c27_g1_i1_g.18742_m.18742  99
293 TRINITY_DN45155_c27_g1_TRINITY_DN45155_c27_g1_i1_g.18742_m.18742 138
401     TRINITY_DN39079_c3_g1_TRINITY_DN39079_c3_g1_i1_g.8354_m.8354 244
406     TRINITY_DN39696_c4_g1_TRINITY_DN39696_c4_g1_i1_g.8926_m.8926 283
    OBS.HOM1.HET.HOM2. E.HOM1.HET.HOM2. ChiSq_HWE        P_HWE P_HET_DEFICIT
291            11/0/13  5.04/11.92/7.04        24 9.114786e-08  9.114786e-08
293             19/0/5  15.04/7.92/1.04        24 6.498371e-06  6.498371e-06
401            13/0/11  7.04/11.92/5.04        24 9.114786e-08  9.114786e-08
406            13/0/11  7.04/11.92/5.04        24 9.114786e-08  9.114786e-08
    P_HET_EXCESS
291            1
293            1
401            1
406            1
```

The "11/0/13" is:
	11 # of homozygotes 1
	0 # heterozygous
	13 # homozygotes 2
These should add up to 24 (number of individuals we have)

Now we are going to calculate disequilibriume

First quite R:
```
quit ()
```

Nevermind it was going to take too long
Instead we are trying genome R squared
```
R
```
```
ld <- read.table("out.geno.ld",header=T)
str(ld)
ld$dist <- abs(ld$POS1-ld$POS2)
str(ld)
```

ld$dist <- abs(ld$POS1-ld$POS2) This calc the absolute value of these two columns to calc the distance



------

<div id='id-section13'/> 

### Page 13: Notes from meeting with Steve 2017-03-06; HW2  

first variable in design is always control variable 
square brakets lets you select rows and columns (blank = select all) []
bring up in write up that we used 100 reads cut off even thought # of individuals changes (can play around withit)
Basemean = ho dif from mean calc with design
log2fold change = the # times (double) that the gene is more expressed (in sick vs health with model 1)


```
> library("ggplot2")
> 
> countsTable <- read.delim('countsdata_trim2.txt', header=TRUE, stringsAsFactors=TRUE, row.names=1)
> countData <- as.matrix(countsTable)
> View(countsTable)
> View(countsTable)
> conds <- read.delim("cols_data_trim.txt", header=TRUE, stringsAsFactors=TRUE, row.names=1)
> head(conds)
             indiv   day health score location
I03_5.08_S_2   I03 day03      S     2      int
I03_5.11_S_4   I03 day06      S     4      int
I07_5.08_S_1   I07 day03      S     1      int
I08_5.08_H_0   I08 day03      H     0      int
I08_5.11_S_1   I08 day06      S     1      int
I08_5.14_S_1   I08 day09      S     1      int
> colData <- as.data.frame(conds)
> View(colData)
> dim(countData)
[1] 13053    77
```
Everything so far has been normal; we read in the full column and counts tables to be used.
Now we need to trim the col and count tables to include ONLY intertidal data (int) and then again so we have ones with ONLY subtidal (sub) data
First you need to go into the text file (manually)
	You will notice they are already organized so that the individuals you see first happen to be intertidal and second are subtital
	Therfore, we can count the number of rows that have intertidal (48)
	Now we know to trim the file so that rows 1-48 are all you have in your intertidal file and rows 49-77 are all you have in your subtidal file
	Since we still want all the other data we leave the other part of the [] empty but seperated with a ,
```
> count_int = countData[,1:48]
> dim(count_int)
[1] 13053    48
```
See now we only have 48 individuals in the int file vs 77
```
> View(count_int)
> count_sub = countData[,49:77]
> dim(count_sub)
[1] 13053    29
```
now we only have 29 individuals in the subtidal group 
```
> write.table(count_int,"count_int.txt")
```
The above command saves the table to the desktop.
Since we didn't specify where it will be saved in the file that the R session is currently working in
If you want to specify where you would add the location before the file name within the ""

Now we will do the same type of triming to the col data
```
> col_int = colData[1:48,]
> dim(col_int)
[1] 48  5
> col_sub = colData[49:77,]
> dim(col_sub)
[1] 29  5
```
NOW we can use the files we trimed to start with the intertidal health analysis
the design always need ~ (see hand written notes from meeting for more detail)

```
> dds_int <- DESeqDataSetFromMatrix(countData = count_int, colData = col_int, design = ~ health)
> dim(dds_int)
[1] 13053    48
```
The above command got the tables and design ready to run
The below command will set a filter to say we only want genes that have 100 or more reads
	**This cut off can change based on the number of individuals etc. play around with the cut off and see how many genes you lose each time
```
> dds_int <- dds_int[ rowSums(counts(dds_int)) > 100, ]
> dim(dds_int)
[1] 12405    48
```
We lost a few more genes by keeping the 100 reads cut off then when we ran the health using location as a contorl analysis

```
> colData(dds_int)$health <- factor(colData(dds_int)$health, levels=c("H","S"))
```
the above line sets health as the reference
the below line FINALLY runs the DESeq using all the conditions we just set up through the previous commands

```
> dds_int <- DESeq(dds_int)
estimating size factors
estimating dispersions
gene-wise dispersion estimates
mean-dispersion relationship
final dispersion estimates
fitting model and testing
-- replacing outliers and refitting for 1529 genes
-- DESeq argument 'minReplicatesForReplace' = 7 
-- original counts are preserved in counts(dds)
estimating dispersions
fitting model and testing
```
The following lines take the results and organize them by adjusted p value (padj) then prints the head of the file so we can look at it

```
> res_int <- results(dds_int)
> res_int <- res_int[order(res_int$padj),]
> head(res_int)
log2 fold change (MAP): health S vs H 
Wald test p-value: health S vs H 
DataFrame with 6 rows and 6 columns
                                                                baseMean log2FoldChange     lfcSE
                                                               <numeric>      <numeric> <numeric>
TRINITY_DN43080_c1_g1_TRINITY_DN43080_c1_g1_i3_g.14110_m.14110 1329.5134       2.646091 0.4276262
TRINITY_DN45253_c3_g1_TRINITY_DN45253_c3_g1_i2_g.18927_m.18927 1351.4485       1.819147 0.3311362
TRINITY_DN43359_c0_g1_TRINITY_DN43359_c0_g1_i1_g.14658_m.14658  849.5637       1.258881 0.2413130
TRINITY_DN46589_c0_g1_TRINITY_DN46589_c0_g1_i1_g.22741_m.22741  434.6755       1.958255 0.3726593
TRINITY_DN46844_c0_g1_TRINITY_DN46844_c0_g1_i3_g.23563_m.23563  471.0032       2.485896 0.4905046
TRINITY_DN43023_c0_g1_TRINITY_DN43023_c0_g1_i1_g.14016_m.14016  345.5715       2.284716 0.4542991
                                                                    stat       pvalue         padj
                                                               <numeric>    <numeric>    <numeric>
TRINITY_DN43080_c1_g1_TRINITY_DN43080_c1_g1_i3_g.14110_m.14110  6.187859 6.098700e-10 1.844247e-06
TRINITY_DN45253_c3_g1_TRINITY_DN45253_c3_g1_i2_g.18927_m.18927  5.493653 3.937030e-08 5.952790e-05
TRINITY_DN43359_c0_g1_TRINITY_DN43359_c0_g1_i1_g.14658_m.14658  5.216799 1.820412e-07 1.376232e-04
TRINITY_DN46589_c0_g1_TRINITY_DN46589_c0_g1_i1_g.22741_m.22741  5.254813 1.481747e-07 1.376232e-04
TRINITY_DN46844_c0_g1_TRINITY_DN46844_c0_g1_i3_g.23563_m.23563  5.068037 4.019384e-07 2.430924e-04
TRINITY_DN43023_c0_g1_TRINITY_DN43023_c0_g1_i1_g.14016_m.14016  5.029100 4.927866e-07 2.483645e-04
```
Finally, we look at the summary for the intsick vs inthealthy:
```
> summary(res_int)

out of 12399 with nonzero total read count
adjusted p-value < 0.1
LFC > 0 (up)     : 205, 1.7% 
LFC < 0 (down)   : 37, 0.3% 
outliers [1]     : 0, 0% 
low counts [2]   : 9381, 76% 
(mean count < 41)
[1] see 'cooksCutoff' argument of ?results
[2] see 'independentFiltering' argument of ?results
```

------


<div id='id-section14'/> 

### Page 14: Script for Homework #2; 2017-03-08  

#### Code from 3/3/17 run at 3:55pm; health with location as control

Basically I ran model #1 as is:
```
setwd("C:/Users/Hannah/Desktop/DGE data from 2-27")
> library("DESeq2")
> 
> library("ggplot2")
> 
> countsTable <- read.delim('countsdata_trim2.txt', header=TRUE, stringsAsFactors=TRUE, row.names=1)
> countData <- as.matrix(countsTable)
> head(countData)
                                                           I03_5.08_S_2 I03_5.11_S_4 I07_5.08_S_1 I08_5.08_H_0
TRINITY_DN10003_c0_g1_TRINITY_DN10003_c0_g1_i1_g.850_m.850            4           26          246            0
TRINITY_DN10009_c0_g1_TRINITY_DN10009_c0_g1_i1_g.851_m.851           17            0            0            0
TRINITY_DN1000_c0_g1_TRINITY_DN1000_c0_g1_i1_g.101_m.101              0           11          262           55
TRINITY_DN10011_c0_g1_TRINITY_DN10011_c0_g1_i1_g.852_m.852           34            0           61            0
TRINITY_DN10021_c0_g1_TRINITY_DN10021_c0_g1_i1_g.853_m.853           34            5            0            0
TRINITY_DN10033_c0_g1_TRINITY_DN10033_c0_g1_i1_g.855_m.855          141           85            0           29
                                                           I08_5.11_S_1 I08_5.14_S_1 I08_5.17_S_2 I08_5.20_S_3
TRINITY_DN10003_c0_g1_TRINITY_DN10003_c0_g1_i1_g.850_m.850            0           68           26            0
TRINITY_DN10009_c0_g1_TRINITY_DN10009_c0_g1_i1_g.851_m.851            0            0           21            0
TRINITY_DN1000_c0_g1_TRINITY_DN1000_c0_g1_i1_g.101_m.101              0            0            0            2
TRINITY_DN10011_c0_g1_TRINITY_DN10011_c0_g1_i1_g.852_m.852            0            0           34            0
TRINITY_DN10021_c0_g1_TRINITY_DN10021_c0_g1_i1_g.853_m.853            0            0            0            0
TRINITY_DN10033_c0_g1_TRINITY_DN10033_c0_g1_i1_g.855_m.855            0           76           64            0
                                                           I09_5.08_H_0 I09_5.14_S_2 I09_5.17_S_2 I09_5.20_S_5
TRINITY_DN10003_c0_g1_TRINITY_DN10003_c0_g1_i1_g.850_m.850            0            0            0            0
TRINITY_DN10009_c0_g1_TRINITY_DN10009_c0_g1_i1_g.851_m.851            0            0           69            0
TRINITY_DN1000_c0_g1_TRINITY_DN1000_c0_g1_i1_g.101_m.101              0            0            0            1
TRINITY_DN10011_c0_g1_TRINITY_DN10011_c0_g1_i1_g.852_m.852            0            0            0            6
TRINITY_DN10021_c0_g1_TRINITY_DN10021_c0_g1_i1_g.853_m.853            0            3            0            0
TRINITY_DN10033_c0_g1_TRINITY_DN10033_c0_g1_i1_g.855_m.855           10            6           99           28
                                                           I10_5.08_H_0 I10_5.11_H_0 I10_5.14_H_0 I10_5.17_H_0
TRINITY_DN10003_c0_g1_TRINITY_DN10003_c0_g1_i1_g.850_m.850            0            0            0            0
TRINITY_DN10009_c0_g1_TRINITY_DN10009_c0_g1_i1_g.851_m.851            0            0            4            0
TRINITY_DN1000_c0_g1_TRINITY_DN1000_c0_g1_i1_g.101_m.101              0            0            0          194
TRINITY_DN10011_c0_g1_TRINITY_DN10011_c0_g1_i1_g.852_m.852            0            0            0            0
TRINITY_DN10021_c0_g1_TRINITY_DN10021_c0_g1_i1_g.853_m.853            0            0            2            0
TRINITY_DN10033_c0_g1_TRINITY_DN10033_c0_g1_i1_g.855_m.855            0            0            0          439
                                                           I10_5.20_S_2 I14_5.08_S_2 I14_5.11_S_2 I15_5.08_H_0
TRINITY_DN10003_c0_g1_TRINITY_DN10003_c0_g1_i1_g.850_m.850            0           20            9            0
TRINITY_DN10009_c0_g1_TRINITY_DN10009_c0_g1_i1_g.851_m.851            0           27            0            0
TRINITY_DN1000_c0_g1_TRINITY_DN1000_c0_g1_i1_g.101_m.101              0            0            0            0
TRINITY_DN10011_c0_g1_TRINITY_DN10011_c0_g1_i1_g.852_m.852            0            0            0           58
TRINITY_DN10021_c0_g1_TRINITY_DN10021_c0_g1_i1_g.853_m.853            0           59            0            0
TRINITY_DN10033_c0_g1_TRINITY_DN10033_c0_g1_i1_g.855_m.855           61           14           11            0
                                                           I15_5.11_H_0 I15_5.14_H_0 I15_5.20_S_3 I19_5.11_H_0
TRINITY_DN10003_c0_g1_TRINITY_DN10003_c0_g1_i1_g.850_m.850           15           16            0            0
TRINITY_DN10009_c0_g1_TRINITY_DN10009_c0_g1_i1_g.851_m.851           14            0            0           40
TRINITY_DN1000_c0_g1_TRINITY_DN1000_c0_g1_i1_g.101_m.101              0            5            0            0
TRINITY_DN10011_c0_g1_TRINITY_DN10011_c0_g1_i1_g.852_m.852            0           10            0            0
TRINITY_DN10021_c0_g1_TRINITY_DN10021_c0_g1_i1_g.853_m.853            0           14           17            0
TRINITY_DN10033_c0_g1_TRINITY_DN10033_c0_g1_i1_g.855_m.855           64          100           48           85
                                                           I19_5.14_H_0 I19_5.17_H_0 I19_5.20_S_5 I20_5.08_H_0
TRINITY_DN10003_c0_g1_TRINITY_DN10003_c0_g1_i1_g.850_m.850            0            0           38            0
TRINITY_DN10009_c0_g1_TRINITY_DN10009_c0_g1_i1_g.851_m.851           30            0            0            0
TRINITY_DN1000_c0_g1_TRINITY_DN1000_c0_g1_i1_g.101_m.101              0            0            0            0
TRINITY_DN10011_c0_g1_TRINITY_DN10011_c0_g1_i1_g.852_m.852           19            0            0            0
TRINITY_DN10021_c0_g1_TRINITY_DN10021_c0_g1_i1_g.853_m.853           22           10           27            0
TRINITY_DN10033_c0_g1_TRINITY_DN10033_c0_g1_i1_g.855_m.855            1            0           52            0
                                                           I20_5.11_H_0 I20_5.14_H_0 I20_5.17_H_0 I20_5.20_S_2
TRINITY_DN10003_c0_g1_TRINITY_DN10003_c0_g1_i1_g.850_m.850           36            0            0           34
TRINITY_DN10009_c0_g1_TRINITY_DN10009_c0_g1_i1_g.851_m.851           10            0            0            0
TRINITY_DN1000_c0_g1_TRINITY_DN1000_c0_g1_i1_g.101_m.101              6            0            0            0
TRINITY_DN10011_c0_g1_TRINITY_DN10011_c0_g1_i1_g.852_m.852           19            0            0            0
TRINITY_DN10021_c0_g1_TRINITY_DN10021_c0_g1_i1_g.853_m.853           18            0            0            0
TRINITY_DN10033_c0_g1_TRINITY_DN10033_c0_g1_i1_g.855_m.855           47           14            2            0
                                                           I22_5.08_S_1 I24_5.11_H_0 I24_5.14_H_0 I24_5.17_H_0
TRINITY_DN10003_c0_g1_TRINITY_DN10003_c0_g1_i1_g.850_m.850            0            0           24            0
TRINITY_DN10009_c0_g1_TRINITY_DN10009_c0_g1_i1_g.851_m.851            0            0            0            0
TRINITY_DN1000_c0_g1_TRINITY_DN1000_c0_g1_i1_g.101_m.101              0            0           18            2
TRINITY_DN10011_c0_g1_TRINITY_DN10011_c0_g1_i1_g.852_m.852            0            0           16            0
TRINITY_DN10021_c0_g1_TRINITY_DN10021_c0_g1_i1_g.853_m.853            0            0            0            0
TRINITY_DN10033_c0_g1_TRINITY_DN10033_c0_g1_i1_g.855_m.855            2            0           26            0
                                                           I26_5.08_S_2 I26_5.11_S_3 I27_5.08_H_0 I27_5.11_H_0
TRINITY_DN10003_c0_g1_TRINITY_DN10003_c0_g1_i1_g.850_m.850            0           16            0            2
TRINITY_DN10009_c0_g1_TRINITY_DN10009_c0_g1_i1_g.851_m.851            0            0            8           30
TRINITY_DN1000_c0_g1_TRINITY_DN1000_c0_g1_i1_g.101_m.101              0            0            0            4
TRINITY_DN10011_c0_g1_TRINITY_DN10011_c0_g1_i1_g.852_m.852            0            0            0           11
TRINITY_DN10021_c0_g1_TRINITY_DN10021_c0_g1_i1_g.853_m.853            0            0           16            9
TRINITY_DN10033_c0_g1_TRINITY_DN10033_c0_g1_i1_g.855_m.855            0           43            5           13
                                                           I27_5.14_H_0 I27_5.20_H_0 I28_5.08_S_1 I28_5.11_S_1
TRINITY_DN10003_c0_g1_TRINITY_DN10003_c0_g1_i1_g.850_m.850            0            0            2            0
TRINITY_DN10009_c0_g1_TRINITY_DN10009_c0_g1_i1_g.851_m.851            0            0           18            0
TRINITY_DN1000_c0_g1_TRINITY_DN1000_c0_g1_i1_g.101_m.101              0            0            0            0
TRINITY_DN10011_c0_g1_TRINITY_DN10011_c0_g1_i1_g.852_m.852            0           20            0            0
TRINITY_DN10021_c0_g1_TRINITY_DN10021_c0_g1_i1_g.853_m.853            0            0            0            0
TRINITY_DN10033_c0_g1_TRINITY_DN10033_c0_g1_i1_g.855_m.855            0            0            0           38
                                                           I28_5.14_S_2 I29_5.08_S_2 I29_5.11_S_2 I29_5.14_S_2
TRINITY_DN10003_c0_g1_TRINITY_DN10003_c0_g1_i1_g.850_m.850           14            0            0           20
TRINITY_DN10009_c0_g1_TRINITY_DN10009_c0_g1_i1_g.851_m.851           47           18            0            8
TRINITY_DN1000_c0_g1_TRINITY_DN1000_c0_g1_i1_g.101_m.101              0            0            0            0
TRINITY_DN10011_c0_g1_TRINITY_DN10011_c0_g1_i1_g.852_m.852            0            0            0            0
TRINITY_DN10021_c0_g1_TRINITY_DN10021_c0_g1_i1_g.853_m.853            0           23           20            0
TRINITY_DN10033_c0_g1_TRINITY_DN10033_c0_g1_i1_g.855_m.855           89           10            0           16
                                                           I31_6.12_H_0 I31_6.15_H_0 I31_6.18_H_0 I31_6.21_H_0
TRINITY_DN10003_c0_g1_TRINITY_DN10003_c0_g1_i1_g.850_m.850            0            0            0            0
TRINITY_DN10009_c0_g1_TRINITY_DN10009_c0_g1_i1_g.851_m.851            0            0            0            0
TRINITY_DN1000_c0_g1_TRINITY_DN1000_c0_g1_i1_g.101_m.101             34            0           27            0
TRINITY_DN10011_c0_g1_TRINITY_DN10011_c0_g1_i1_g.852_m.852            0            0          119            0
TRINITY_DN10021_c0_g1_TRINITY_DN10021_c0_g1_i1_g.853_m.853           11            0           74            0
TRINITY_DN10033_c0_g1_TRINITY_DN10033_c0_g1_i1_g.855_m.855           33           58            1           26
                                                           I31_6.24_H_0 I32_6.12_H_0 I32_6.15_H_0 I32_6.18_H_0
TRINITY_DN10003_c0_g1_TRINITY_DN10003_c0_g1_i1_g.850_m.850           33           10            3            2
TRINITY_DN10009_c0_g1_TRINITY_DN10009_c0_g1_i1_g.851_m.851           50           49            8           13
TRINITY_DN1000_c0_g1_TRINITY_DN1000_c0_g1_i1_g.101_m.101              0            0           10           18
TRINITY_DN10011_c0_g1_TRINITY_DN10011_c0_g1_i1_g.852_m.852           44            4           14            0
TRINITY_DN10021_c0_g1_TRINITY_DN10021_c0_g1_i1_g.853_m.853            0            0            0            5
TRINITY_DN10033_c0_g1_TRINITY_DN10033_c0_g1_i1_g.855_m.855            0           13           46            0
                                                           I32_6.21_H_0 I33_6.12_H_0 I33_6.15_H_0 I33_6.18_H_0
TRINITY_DN10003_c0_g1_TRINITY_DN10003_c0_g1_i1_g.850_m.850            0            0            0           74
TRINITY_DN10009_c0_g1_TRINITY_DN10009_c0_g1_i1_g.851_m.851            0           46            0           29
TRINITY_DN1000_c0_g1_TRINITY_DN1000_c0_g1_i1_g.101_m.101              0            0           24            6
TRINITY_DN10011_c0_g1_TRINITY_DN10011_c0_g1_i1_g.852_m.852            0           11            0            0
TRINITY_DN10021_c0_g1_TRINITY_DN10021_c0_g1_i1_g.853_m.853            0            0            0            0
TRINITY_DN10033_c0_g1_TRINITY_DN10033_c0_g1_i1_g.855_m.855            0           36           37           26
                                                           I33_6.21_H_0 I33_6.24_H_0 I34_6.12_H_0 I34_6.15_H_0
TRINITY_DN10003_c0_g1_TRINITY_DN10003_c0_g1_i1_g.850_m.850           11            0            0            6
TRINITY_DN10009_c0_g1_TRINITY_DN10009_c0_g1_i1_g.851_m.851            4            0            0            4
TRINITY_DN1000_c0_g1_TRINITY_DN1000_c0_g1_i1_g.101_m.101             12           26           56            2
TRINITY_DN10011_c0_g1_TRINITY_DN10011_c0_g1_i1_g.852_m.852            0           29           69           20
TRINITY_DN10021_c0_g1_TRINITY_DN10021_c0_g1_i1_g.853_m.853            6            0            0           16
TRINITY_DN10033_c0_g1_TRINITY_DN10033_c0_g1_i1_g.855_m.855           17          112           40           59
                                                           I34_6.18_H_0 I34_6.21_H_0 I35_6.12_H_0 I35_6.15_H_0
TRINITY_DN10003_c0_g1_TRINITY_DN10003_c0_g1_i1_g.850_m.850            0            0            5            0
TRINITY_DN10009_c0_g1_TRINITY_DN10009_c0_g1_i1_g.851_m.851           44           20            0            0
TRINITY_DN1000_c0_g1_TRINITY_DN1000_c0_g1_i1_g.101_m.101              0            0           30           62
TRINITY_DN10011_c0_g1_TRINITY_DN10011_c0_g1_i1_g.852_m.852            0            0            6            0
TRINITY_DN10021_c0_g1_TRINITY_DN10021_c0_g1_i1_g.853_m.853            0            0            6            0
TRINITY_DN10033_c0_g1_TRINITY_DN10033_c0_g1_i1_g.855_m.855          102            6           18            0
                                                           I35_6.18_H_0 I35_6.21_H_0 I36_6.12_S_1 I36_6.15_S_2
TRINITY_DN10003_c0_g1_TRINITY_DN10003_c0_g1_i1_g.850_m.850            1            0            0            0
TRINITY_DN10009_c0_g1_TRINITY_DN10009_c0_g1_i1_g.851_m.851           54           15            0            0
TRINITY_DN1000_c0_g1_TRINITY_DN1000_c0_g1_i1_g.101_m.101             14            0           28            0
TRINITY_DN10011_c0_g1_TRINITY_DN10011_c0_g1_i1_g.852_m.852            6            8            0           88
TRINITY_DN10021_c0_g1_TRINITY_DN10021_c0_g1_i1_g.853_m.853            9           20           14            0
TRINITY_DN10033_c0_g1_TRINITY_DN10033_c0_g1_i1_g.855_m.855            8           26           24            0
                                                           I36_6.18_S_3 I37_6.12_H_0 I37_6.15_S_1 I37_6.18_H_0
TRINITY_DN10003_c0_g1_TRINITY_DN10003_c0_g1_i1_g.850_m.850           22           20           12            7
TRINITY_DN10009_c0_g1_TRINITY_DN10009_c0_g1_i1_g.851_m.851            0           57            0          111
TRINITY_DN1000_c0_g1_TRINITY_DN1000_c0_g1_i1_g.101_m.101              9            5            0           54
TRINITY_DN10011_c0_g1_TRINITY_DN10011_c0_g1_i1_g.852_m.852            7           16            0            0
TRINITY_DN10021_c0_g1_TRINITY_DN10021_c0_g1_i1_g.853_m.853           16           49            0            0
TRINITY_DN10033_c0_g1_TRINITY_DN10033_c0_g1_i1_g.855_m.855           10           22            0            8
                                                           I38_6.12_H_0
TRINITY_DN10003_c0_g1_TRINITY_DN10003_c0_g1_i1_g.850_m.850           10
TRINITY_DN10009_c0_g1_TRINITY_DN10009_c0_g1_i1_g.851_m.851            6
TRINITY_DN1000_c0_g1_TRINITY_DN1000_c0_g1_i1_g.101_m.101              7
TRINITY_DN10011_c0_g1_TRINITY_DN10011_c0_g1_i1_g.852_m.852           22
TRINITY_DN10021_c0_g1_TRINITY_DN10021_c0_g1_i1_g.853_m.853            2
TRINITY_DN10033_c0_g1_TRINITY_DN10033_c0_g1_i1_g.855_m.855           10
> 
> conds <- read.delim("cols_data_trim.txt", header=TRUE, stringsAsFactors=TRUE, row.names=1)
> head(conds)
             indiv   day health score location
I03_5.08_S_2   I03 day03      S     2      int
I03_5.11_S_4   I03 day06      S     4      int
I07_5.08_S_1   I07 day03      S     1      int
I08_5.08_H_0   I08 day03      H     0      int
I08_5.11_S_1   I08 day06      S     1      int
I08_5.14_S_1   I08 day09      S     1      int
> colData <- as.data.frame(conds)
> head(colData)
             indiv   day health score location
I03_5.08_S_2   I03 day03      S     2      int
I03_5.11_S_4   I03 day06      S     4      int
I07_5.08_S_1   I07 day03      S     1      int
I08_5.08_H_0   I08 day03      H     0      int
I08_5.11_S_1   I08 day06      S     1      int
I08_5.14_S_1   I08 day09      S     1      int
> dds <- DESeqDataSetFromMatrix(countData = countData, colData = colData, design = ~ location + health)
> dim(dds)
[1] 13053    77
> dds <- dds[ rowSums(counts(dds)) > 100, ]
> colData(dds)$health <- factor(colData(dds)$health, levels=c("H","S")) #sets that "healthy is the reference
> dds <- DESeq(dds)
estimating size factors
estimating dispersions
gene-wise dispersion estimates
mean-dispersion relationship
final dispersion estimates
fitting model and testing
-- replacing outliers and refitting for 1051 genes
-- DESeq argument 'minReplicatesForReplace' = 7 
-- original counts are preserved in counts(dds)
estimating dispersions
fitting model and testing
> res <- results(dds)
> res <- res[order(res$padj),]
> head(res)
log2 fold change (MAP): health S vs H 
Wald test p-value: health S vs H 
DataFrame with 6 rows and 6 columns
                                                                baseMean log2FoldChange     lfcSE
                                                               <numeric>      <numeric> <numeric>
TRINITY_DN43080_c1_g1_TRINITY_DN43080_c1_g1_i3_g.14110_m.14110 1105.7751       2.501124 0.3832201
TRINITY_DN42378_c1_g1_TRINITY_DN42378_c1_g1_i2_g.12752_m.12752  172.5713       3.255263 0.5164368
TRINITY_DN45416_c4_g2_TRINITY_DN45416_c4_g2_i3_g.19333_m.19333 1501.8405       2.050527 0.3404161
TRINITY_DN43359_c0_g1_TRINITY_DN43359_c0_g1_i1_g.14658_m.14658  802.6078       1.173162 0.2019297
TRINITY_DN46136_c0_g1_TRINITY_DN46136_c0_g1_i2_g.21366_m.21366  729.2029       2.534860 0.4405170
TRINITY_DN46589_c0_g1_TRINITY_DN46589_c0_g1_i1_g.22741_m.22741  379.4915       1.813462 0.3140506
                                                                    stat       pvalue         padj
                                                               <numeric>    <numeric>    <numeric>
TRINITY_DN43080_c1_g1_TRINITY_DN43080_c1_g1_i3_g.14110_m.14110  6.526600 6.727945e-11 3.279873e-07
TRINITY_DN42378_c1_g1_TRINITY_DN42378_c1_g1_i2_g.12752_m.12752  6.303313 2.913489e-10 7.101629e-07
TRINITY_DN45416_c4_g2_TRINITY_DN45416_c4_g2_i3_g.19333_m.19333  6.023590 1.705906e-09 2.772097e-06
TRINITY_DN43359_c0_g1_TRINITY_DN43359_c0_g1_i1_g.14658_m.14658  5.809755 6.256419e-09 7.069528e-06
TRINITY_DN46136_c0_g1_TRINITY_DN46136_c0_g1_i2_g.21366_m.21366  5.754284 8.700957e-09 7.069528e-06
TRINITY_DN46589_c0_g1_TRINITY_DN46589_c0_g1_i1_g.22741_m.22741  5.774428 7.721500e-09 7.069528e-06
> summary(res)

out of 12947 with nonzero total read count
adjusted p-value < 0.1
LFC > 0 (up)     : 209, 1.6% 
LFC < 0 (down)   : 65, 0.5% 
outliers [1]     : 400, 3.1% 
low counts [2]   : 7679, 59% 
(mean count < 23)
[1] see 'cooksCutoff' argument of ?results
[2] see 'independentFiltering' argument of ?results
```

**
LFC > 0 (up)     : 209, 1.6%  **SICK** 
LFC < 0 (down)   : 65, 0.5%   **HEALTHY**

The above data means that **209** gene are more highly expressed in **Sick** and ***65*** genes were more highly expressed in ***healthy***



#### Code from 3-6-17; intS vs intH; subS vs subH

```
> library("ggplot2")
> 
> countsTable <- read.delim('countsdata_trim2.txt', header=TRUE, stringsAsFactors=TRUE, row.names=1)
> countData <- as.matrix(countsTable)
> View(countsTable)
> View(countsTable)
> conds <- read.delim("cols_data_trim.txt", header=TRUE, stringsAsFactors=TRUE, row.names=1)
> head(conds)
             indiv   day health score location
I03_5.08_S_2   I03 day03      S     2      int
I03_5.11_S_4   I03 day06      S     4      int
I07_5.08_S_1   I07 day03      S     1      int
I08_5.08_H_0   I08 day03      H     0      int
I08_5.11_S_1   I08 day06      S     1      int
I08_5.14_S_1   I08 day09      S     1      int
> colData <- as.data.frame(conds)
> View(colData)
> dim(countData)
[1] 13053    77
```
Everything so far has been normal; we read in the full column and counts tables to be used.
Now we need to trim the col and count tables to include ONLY intertidal data (int) and then again so we have ones with ONLY subtidal (sub) data
First you need to go into the text file (manually)
You will notice they are already organized so that the individuals you see first happen to be intertidal and second are subtital
Therfore, we can count the number of rows that have intertidal (48)
Now we know to trim the file so that rows 1-48 are all you have in your intertidal file and rows 49-77 are all you have in your subtidal file
Since we still want all the other data we leave the other part of the [] empty but seperated with a ,
```
> count_int = countData[,1:48]
> dim(count_int)
[1] 13053    48
```
See now we only have 48 individuals in the int file vs 77
```
> View(count_int)
> count_sub = countData[,49:77]
> dim(count_sub)
[1] 13053    29
```
now we only have 29 individuals in the subtidal group 
Now we will do the same type of triming to the col data
```
> col_int = colData[1:48,]
> dim(col_int)
[1] 48  5
> col_sub = colData[49:77,]
> dim(col_sub)
[1] 29  5
```
NOW we can use the files we trimed to start with the intertidal health analysis
```
> dds_int <- DESeqDataSetFromMatrix(countData = count_int, colData = col_int, design = ~ health)
> dim(dds_int)
[1] 13053    48
```
The below command will set a filter to say we only want genes that have 100 or more reads
	**This cut off can change based on the number of individuals etc. play around with the cut off and see how many genes you lose each time
```
> dds_int <- dds_int[ rowSums(counts(dds_int)) > 100, ]
> dim(dds_int)
[1] 12405    48
```
We lost a few more genes by keeping the 100 reads cut off then when we ran the health using location as a contorl analysis
Now we will set health as the reference and run the DESeq analysis 
```
> colData(dds_int)$health <- factor(colData(dds_int)$health, levels=c("H","S"))
> dds_int <- DESeq(dds_int)
estimating size factors
estimating dispersions
gene-wise dispersion estimates
mean-dispersion relationship
final dispersion estimates
fitting model and testing
-- replacing outliers and refitting for 1529 genes
-- DESeq argument 'minReplicatesForReplace' = 7 
-- original counts are preserved in counts(dds)
estimating dispersions
fitting model and testing
> res_int <- results(dds_int)
> res_int <- res_int[order(res_int$padj),]
> head(res_int)
log2 fold change (MAP): health S vs H 
Wald test p-value: health S vs H 
DataFrame with 6 rows and 6 columns
                                                                baseMean log2FoldChange     lfcSE
                                                               <numeric>      <numeric> <numeric>
TRINITY_DN43080_c1_g1_TRINITY_DN43080_c1_g1_i3_g.14110_m.14110 1329.5134       2.646091 0.4276262
TRINITY_DN45253_c3_g1_TRINITY_DN45253_c3_g1_i2_g.18927_m.18927 1351.4485       1.819147 0.3311362
TRINITY_DN43359_c0_g1_TRINITY_DN43359_c0_g1_i1_g.14658_m.14658  849.5637       1.258881 0.2413130
TRINITY_DN46589_c0_g1_TRINITY_DN46589_c0_g1_i1_g.22741_m.22741  434.6755       1.958255 0.3726593
TRINITY_DN46844_c0_g1_TRINITY_DN46844_c0_g1_i3_g.23563_m.23563  471.0032       2.485896 0.4905046
TRINITY_DN43023_c0_g1_TRINITY_DN43023_c0_g1_i1_g.14016_m.14016  345.5715       2.284716 0.4542991
                                                                    stat       pvalue         padj
                                                               <numeric>    <numeric>    <numeric>
TRINITY_DN43080_c1_g1_TRINITY_DN43080_c1_g1_i3_g.14110_m.14110  6.187859 6.098700e-10 1.844247e-06
TRINITY_DN45253_c3_g1_TRINITY_DN45253_c3_g1_i2_g.18927_m.18927  5.493653 3.937030e-08 5.952790e-05
TRINITY_DN43359_c0_g1_TRINITY_DN43359_c0_g1_i1_g.14658_m.14658  5.216799 1.820412e-07 1.376232e-04
TRINITY_DN46589_c0_g1_TRINITY_DN46589_c0_g1_i1_g.22741_m.22741  5.254813 1.481747e-07 1.376232e-04
TRINITY_DN46844_c0_g1_TRINITY_DN46844_c0_g1_i3_g.23563_m.23563  5.068037 4.019384e-07 2.430924e-04
TRINITY_DN43023_c0_g1_TRINITY_DN43023_c0_g1_i1_g.14016_m.14016  5.029100 4.927866e-07 2.483645e-04
> summary(res_int)

out of 12399 with nonzero total read count
adjusted p-value < 0.1
LFC > 0 (up)     : 205, 1.7% 
LFC < 0 (down)   : 37, 0.3% 
outliers [1]     : 0, 0% 
low counts [2]   : 9381, 76% 
(mean count < 41)
[1] see 'cooksCutoff' argument of ?results
[2] see 'independentFiltering' argument of ?results
```
This concludes that for intS vs intH:
**205** genes were more highly expressed in **sick**
***37*** genes were more highly expressed in ***healthy***
​	
​	
​	
**NOW** to run the subtidal analysis:

```
> dds_sub <- DESeqDataSetFromMatrix(countData = count_sub, colData = col_sub, design = ~ health)
> dim(dds_sub)
[1] 13053    29
> dds_sub <- dds_sub[ rowSums(counts(dds_sub)) > 100, ]
> dim(dds_sub)
[1] 12397    29
> colData(dds_sub)$health <- factor(colData(dds_sub)$health, levels=c("H","S"))
> dds_sub <- DESeq(dds_sub)
estimating size factors
estimating dispersions
gene-wise dispersion estimates
mean-dispersion relationship
final dispersion estimates
fitting model and testing
-- replacing outliers and refitting for 1052 genes
-- DESeq argument 'minReplicatesForReplace' = 7 
-- original counts are preserved in counts(dds)
estimating dispersions
fitting model and testing
> res_sub <- results(dds_sub)
> res_sub <- res_sub[order(res_sub$padj),]
> head(res_sub)
log2 fold change (MAP): health S vs H 
Wald test p-value: health S vs H 
DataFrame with 6 rows and 6 columns
                                                                baseMean
                                                               <numeric>
TRINITY_DN42073_c0_g1_TRINITY_DN42073_c0_g1_i1_g.12173_m.12173  37.90189
TRINITY_DN29480_c0_g1_TRINITY_DN29480_c0_g1_i1_g.3557_m.3557    26.56170
TRINITY_DN46933_c2_g1_TRINITY_DN46933_c2_g1_i1_g.23988_m.23988 163.41417
TRINITY_DN31786_c0_g1_TRINITY_DN31786_c0_g1_i1_g.4198_m.4198    32.42672
TRINITY_DN43786_c1_g1_TRINITY_DN43786_c1_g1_i1_g.15493_m.15493 140.59010
TRINITY_DN40724_c3_g1_TRINITY_DN40724_c3_g1_i1_g.10214_m.10214  45.06837
                                                               log2FoldChange
                                                                    <numeric>
TRINITY_DN42073_c0_g1_TRINITY_DN42073_c0_g1_i1_g.12173_m.12173      -5.731428
TRINITY_DN29480_c0_g1_TRINITY_DN29480_c0_g1_i1_g.3557_m.3557        -4.295459
TRINITY_DN46933_c2_g1_TRINITY_DN46933_c2_g1_i1_g.23988_m.23988      -3.155248
TRINITY_DN31786_c0_g1_TRINITY_DN31786_c0_g1_i1_g.4198_m.4198        -5.029507
TRINITY_DN43786_c1_g1_TRINITY_DN43786_c1_g1_i1_g.15493_m.15493      -3.026674
TRINITY_DN40724_c3_g1_TRINITY_DN40724_c3_g1_i1_g.10214_m.10214      -4.584825
                                                                   lfcSE
                                                               <numeric>
TRINITY_DN42073_c0_g1_TRINITY_DN42073_c0_g1_i1_g.12173_m.12173 0.9113462
TRINITY_DN29480_c0_g1_TRINITY_DN29480_c0_g1_i1_g.3557_m.3557   0.7883082
TRINITY_DN46933_c2_g1_TRINITY_DN46933_c2_g1_i1_g.23988_m.23988 0.5811646
TRINITY_DN31786_c0_g1_TRINITY_DN31786_c0_g1_i1_g.4198_m.4198   0.9498760
TRINITY_DN43786_c1_g1_TRINITY_DN43786_c1_g1_i1_g.15493_m.15493 0.6107190
TRINITY_DN40724_c3_g1_TRINITY_DN40724_c3_g1_i1_g.10214_m.10214 0.9381238
                                                                    stat
                                                               <numeric>
TRINITY_DN42073_c0_g1_TRINITY_DN42073_c0_g1_i1_g.12173_m.12173 -6.288969
TRINITY_DN29480_c0_g1_TRINITY_DN29480_c0_g1_i1_g.3557_m.3557   -5.448959
TRINITY_DN46933_c2_g1_TRINITY_DN46933_c2_g1_i1_g.23988_m.23988 -5.429181
TRINITY_DN31786_c0_g1_TRINITY_DN31786_c0_g1_i1_g.4198_m.4198   -5.294909
TRINITY_DN43786_c1_g1_TRINITY_DN43786_c1_g1_i1_g.15493_m.15493 -4.955919
TRINITY_DN40724_c3_g1_TRINITY_DN40724_c3_g1_i1_g.10214_m.10214 -4.887229
                                                                     pvalue
                                                                  <numeric>
TRINITY_DN42073_c0_g1_TRINITY_DN42073_c0_g1_i1_g.12173_m.12173 3.195814e-10
TRINITY_DN29480_c0_g1_TRINITY_DN29480_c0_g1_i1_g.3557_m.3557   5.066560e-08
TRINITY_DN46933_c2_g1_TRINITY_DN46933_c2_g1_i1_g.23988_m.23988 5.661319e-08
TRINITY_DN31786_c0_g1_TRINITY_DN31786_c0_g1_i1_g.4198_m.4198   1.190760e-07
TRINITY_DN43786_c1_g1_TRINITY_DN43786_c1_g1_i1_g.15493_m.15493 7.198939e-07
TRINITY_DN40724_c3_g1_TRINITY_DN40724_c3_g1_i1_g.10214_m.10214 1.022653e-06
                                                                       padj
                                                                  <numeric>
TRINITY_DN42073_c0_g1_TRINITY_DN42073_c0_g1_i1_g.12173_m.12173 2.384397e-06
TRINITY_DN29480_c0_g1_TRINITY_DN29480_c0_g1_i1_g.3557_m.3557   1.407970e-04
TRINITY_DN46933_c2_g1_TRINITY_DN46933_c2_g1_i1_g.23988_m.23988 1.407970e-04
TRINITY_DN31786_c0_g1_TRINITY_DN31786_c0_g1_i1_g.4198_m.4198   2.221065e-04
TRINITY_DN43786_c1_g1_TRINITY_DN43786_c1_g1_i1_g.15493_m.15493 1.074226e-03
TRINITY_DN40724_c3_g1_TRINITY_DN40724_c3_g1_i1_g.10214_m.10214 1.271669e-03
> summary(res_sub)

out of 12392 with nonzero total read count
adjusted p-value < 0.1
LFC > 0 (up)     : 20, 0.16% 
LFC < 0 (down)   : 113, 0.91% 
outliers [1]     : 647, 5.2% 
low counts [2]   : 4289, 35% 
(mean count < 13)
[1] see 'cooksCutoff' argument of ?results
[2] see 'independentFiltering' argument of ?results
```
This concludes that for subS vs subH:
**20** genes were more highly expressed in **sick**
***113*** genes were more highly expressed in ***healthy***



#### Code from 3-7-17; plots

```
> vsd <- varianceStabilizingTransformation(dds_int, blind=FALSE)
> vsd_int <- varianceStabilizingTransformation(dds_int, blind=FALSE)
> plotPCA(vsd_int, intgroup=c("health"))
> vsd_sub <- varianceStabilizingTransformation(dds_sub, blind=FALSE)
> plotPCA(vsd_sub, intgroup=c("health"))
```
I had to re run the health with location as control model since I had exited out of R since I last ran it:
```
> dds <- DESeqDataSetFromMatrix(countData = countData, colData = colData, design = ~ location + health)
> dim(dds)
[1] 13053    77
> dds <- dds[ rowSums(counts(dds)) > 100, ]
> dim(dds)
[1] 12954    77
> colData(dds)$health <- factor(colData(dds)$health, levels=c("H","S")) #sets that "healthy is the reference
> dds <- DESeq(dds)
estimating size factors
estimating dispersions
gene-wise dispersion estimates
mean-dispersion relationship
final dispersion estimates
fitting model and testing
-- replacing outliers and refitting for 1051 genes
-- DESeq argument 'minReplicatesForReplace' = 7 
-- original counts are preserved in counts(dds)
estimating dispersions
fitting model and testing
> res <- results(dds)
> res <- res[order(res$padj),]
> head(res)
log2 fold change (MAP): health S vs H 
Wald test p-value: health S vs H 
DataFrame with 6 rows and 6 columns
                                                                baseMean log2FoldChange     lfcSE
                                                               <numeric>      <numeric> <numeric>
TRINITY_DN43080_c1_g1_TRINITY_DN43080_c1_g1_i3_g.14110_m.14110 1105.7751       2.501124 0.3832201
TRINITY_DN42378_c1_g1_TRINITY_DN42378_c1_g1_i2_g.12752_m.12752  172.5713       3.255263 0.5164368
TRINITY_DN45416_c4_g2_TRINITY_DN45416_c4_g2_i3_g.19333_m.19333 1501.8405       2.050527 0.3404161
TRINITY_DN43359_c0_g1_TRINITY_DN43359_c0_g1_i1_g.14658_m.14658  802.6078       1.173162 0.2019297
TRINITY_DN46136_c0_g1_TRINITY_DN46136_c0_g1_i2_g.21366_m.21366  729.2029       2.534860 0.4405170
TRINITY_DN46589_c0_g1_TRINITY_DN46589_c0_g1_i1_g.22741_m.22741  379.4915       1.813462 0.3140506
                                                                    stat       pvalue         padj
                                                               <numeric>    <numeric>    <numeric>
TRINITY_DN43080_c1_g1_TRINITY_DN43080_c1_g1_i3_g.14110_m.14110  6.526600 6.727945e-11 3.279873e-07
TRINITY_DN42378_c1_g1_TRINITY_DN42378_c1_g1_i2_g.12752_m.12752  6.303313 2.913489e-10 7.101629e-07
TRINITY_DN45416_c4_g2_TRINITY_DN45416_c4_g2_i3_g.19333_m.19333  6.023590 1.705906e-09 2.772097e-06
TRINITY_DN43359_c0_g1_TRINITY_DN43359_c0_g1_i1_g.14658_m.14658  5.809755 6.256419e-09 7.069528e-06
TRINITY_DN46136_c0_g1_TRINITY_DN46136_c0_g1_i2_g.21366_m.21366  5.754284 8.700957e-09 7.069528e-06
TRINITY_DN46589_c0_g1_TRINITY_DN46589_c0_g1_i1_g.22741_m.22741  5.774428 7.721500e-09 7.069528e-06
> summary(res)

out of 12947 with nonzero total read count
adjusted p-value < 0.1
LFC > 0 (up)     : 209, 1.6% 
LFC < 0 (down)   : 65, 0.5% 
outliers [1]     : 400, 3.1% 
low counts [2]   : 7679, 59% 
(mean count < 23)
[1] see 'cooksCutoff' argument of ?results
[2] see 'independentFiltering' argument of ?results
```

Now back to making plots:
```
> vsd <- varianceStabilizingTransformation(dds, blind=FALSE)
> plotPCA(vsd, intgroup=c("health"))
> plotPCA(vsd, intgroup=c("health","location"))
> plotPCA(vsd_int, intgroup=c("health","location"))
> plotPCA(vsd_sub, intgroup=c("health","location"))
```
------

<div id='id-section15'/> 

### Page 15: Notes from command practice in class 2017-03-08; vcftools  

Steve merged all data for each individual into one file and called snps for each of the merged files

To access that file go to:
```
cd /data/project_data/snps/reads2snps
```

We are interested in this file:
```
SSW_byind.txt.vcf.gz
```
gz = zipped; Thats why its red; we can't look into this file (head/tail) because its zipped

telling it that we are going to read in a zipped vcf file when we put "--gzvcf"

```
vcftools --gzvcf SSW_byind.txt.vcf.gz
```
shows that there are 22 individuals and 7485987 SNPs

now we are running the same filters that we did last time except now we apply it to this new merged file

```
vcftools --gzvcf SSW_byind.txt.vcf.gz --min-alleles 2 --max-alleles 2 --maf 0.02 --max-missing 0.8 --recode --out ~/SSW_all_biallelic.MAF0.02.Miss0.8  
```
now we have 5565 SNPs (better then last time where we had about 1,000)
these are the really good, high quality, you can probably trust them data

I went and checked that the file was in my home directory:

```
cd ~/
```
its there!

now we are going to zip up that file

```
gzip SSW_all_biallelic.MAF0.02.Miss0.8.recode.vcf
ll
```
Its there and its red so that means its zipped 

Now we will run a test for hardy winberg equilibrium to see how they are behaving...(?)

```
vcftools --gzvcf SSW_all_biallelic.MAF0.02.Miss0.8.recode.vcf.gz --hardy
```

Ran really fast; 

```
ll
```

You will see two new files with "out"; interesting stuff is in "out.hwe" thats where your test stats are for hardy winberg

Take a look at that file:

```
head out.hwe
```

Not very easy to read
therefore we will open in R (open R on terminal)

```
R
```

read file into R by doing the following:

```
hwe <- read.table("out.hwe", header=T)
```

can look at structure of the data by running the following:

```
str(hwe)
```

last three lines show the p values
P_HWE (global)
P_DEFICIT (less heterozygotes;inbreeding)
P_EXCESS (extra heterozygotes)
Lets get some info on those P values by doing:

```
summary(hwe)
```

we want to know the SNPs that show significatin deviation from HWE
start with DEFICIT

```
which(hwe$P_HET_DEFICIT<0.01)
```

This shows us which rows of HWE deficit are less then 0.01 p value
NOTE: left of $ is data frame right of $ is column you want to investigate
It outputs row numbers (ie 1001 is a row number 1021 is another row number...)

Vcftools looks at what is significantly different from the expected (i think...)

Now we can add more to the which command to get the data in those rows (ie which transcript etc)

```
hwe[which(hwe$P_HET_DEFICIT<0.01),]
```

this will print all of the rows that show a deficit at the condtions we laid out; print JUST those rows and all the columns (since we left that blank)

You notice that there are three Snps in the same gene (same transcript) that affect that allele (makes sense since they are linked)
When looking at the long title (see below) the c27 is a gene (sometimes mentioned as a transcript)

```
TRINITY_DN45155_c27_g2_TRINITY_DN45155_c27_g2_i2_g.18743_m.18743 
```

**What we just did (bringing stuff in/out of R to work with) we should get comfortable with

exit R:

```
quit ()
```

Now we are going to takke a more in-depth look at the diversity hidden within these sea star data
We will start by calculating allele frequency
in order to do comparisons within groups you have to tell it how individuals assigin to each group:
you need a set of input files
files need to have sample IDs and they have to be exactly the same sample IDs used in each file
have a file called "sick.txt" that has all the sick sample IDs and a file called "healthy.txt" with all the healthy individual sample IDs
we decided to put SS and HS in sick.txt and put HH in healthy.txt (leave MM out for now)

Go to where the health location file is that Steve made and then we will seperate them 

```
cd /data/project_data/snps
ll
cd reads2snps
ll
```

look at the SSW_H...

```
(FILE NAME HERE)
```
This file has:
	individual number
	health status (HH, HS, SS, MM)
	Location (INT, SUB)
	SNPs (Y or N)

How can we grab certain columns in this file and export (will take other stuff on line as well; ind location etc)

```
grep "HH" ssw_healthloc.txt >~/H_onesampleperind.txt 
grep "SS" ssw_healthloc.txt >~/S_onesampleperind.txt
```

Now we want to add HS to the end of the H_onesampleperind.txt file so we do this:

```
grep "HS" ssw_healthloc.txt >>~/H_onesampleperind.txt
```

Now lets go check that the file is there and open it up to see if everything we want is in there

```
 cd ~/
 ll
 cat H_onesampleperind.txt
```

we need to trim some stuff we don't need right now out of the file:

```
cut -f 1 H_onesampleperind.txt >H_onesampleperind_trim.txt
```

this will output it to a new file (since I put a new name after the >)
go and look at file:

```
ll
cat H_onesampleperind_trim.txt
```

Good its only the individual ID

now do it for the sick:

```
cut -f 1 S_onesampleperind.txt >S_onesampleperind_trim.txt
ll
cat S_onesampleperind_trim.txt
```

Good; now we are going to use these files to calc allele frequency 

```
vcftools --gzvcf SSW_all_biallelic.MAF0.02.Miss0.8.recode.vcf.gz --freq2 --keep H_onesampleperind_trim.txt --out H_AlleleFreqs
```

Says its only keeping 11 out of 22 individuals...suppose to only keep 6...
Oops I accidentally added the HS to my Healthy file when they should go in the sick file...
I'm going to go in manually and delete the HS ind from my file and add them to the sick file

```
vim H_onesampleperind_trim.txt
i
manually deleted HS
esc
:wq
```

now add the HS

```
vim S_onesampleperind_trim.txt
i 
manuall add the HS 
esc 
:wq
cat S_onesampleperind_trim.txt
```

rerun command that calculates allele freq:

```
vcftools --gzvcf SSW_all_biallelic.MAF0.02.Miss0.8.recode.vcf.gz --freq2 --keep H_onesampleperind_trim.txt --out H_AlleleFreqs
```

YAY! it only kept 6 individuals this time which is what we should have
Now we need to run it for sick:

```
vcftools --gzvcf SSW_all_biallelic.MAF0.02.Miss0.8.recode.vcf.gz --freq2 --keep S_onesampleperind_trim.txt --out S_AlleleFreqs
```

it kept 14 individauls ...is that what we should have?

We need to go into the file it output to change the column header {Freq} to two column headers called "MAJOR" and "MINOR"

```
vim S_AlleleFreqs.frq
i 
delete {Freq} 
add MAJOR (tab) MINOR 
esc 
:wq 
```

do the same to healthy:

```
vim H_AlleleFreqs.frq
i 
delete {Freq} 
add MAJOR (tab) MINOR 
esc 
:wq 
```

Thats it for today


  X
{Freq}
MAJOR (tab) MINOR

------

<div id='id-section16'/> 

### Page 16: Notes from command practice in class 2017-03-20; pop gen  

Commands notes from 3-20-17
1) FINAL VCF data -> filter -> output to HD
2) estimate allele frequency differnece between all H and S SNPs
f(H) - f(S)
2b) FST. between H and S  -> output to local machine -> output to R
3) estimate pie at synon, non snynom, pie nonsyn/pie syn -> output to local machine compare to Romiguirer (life history analysis paper; lost of species)


Start Coding (going to go fast today):

We have all 
```
$ cd /data/project_data/snps/reads2snps
$ vcftools --gzvcf SSW_by24inds.txt.vcf.gz --min-alleles 2 --max-alleles 2 --maf 0.02 --max-missing 0.8 --recode --out ~/SSW_all_biallelic.MAF0.02.Miss0.8  
$ cd ~/
$ gzip SSW_all_biallelic.MAF0.02.Miss0.8..recode.vcf 
```
creating a text file with ids for just H and just S
```
cd /data/project_data/snps/reads2snps/
cat ssw_healthloc.txt
```
going to grab (grep) any HH and only take (cut) the first column; automatical outputs to home directory
```
grep "HH" ssw_healthloc.txt | cut -f1 >~/H_SampleIDs.txt
```
Lets check its there and that there are only 8 file names:
```
cd ~/
ll
cat H_SampleIDs.txt
```
Yup! only 8 =)

We could also use wc (word count); will count # of rows and # of characters in a file:

Now do same for sick individuals (remember we need BOTH HS and SS):
```
cd /data/project_data/snps/reads2snps/
grep "HS\|SS" ssw_healthloc.txt | cut -f1 >~/S_SampleIDs.txt
```
Now go and check that there are 14 individuals
```
cd ~/
ll
cat S_SampleIDs.txt
```
Yup!  We are good to go 


Now we should have everything we need to estimate the frequency of alleles for each SNP (do first for H then for S)
```
vcftools --gzvcf SSW_all_biallelic.MAF0.02.Miss0.8..recode.vcf.gz --freq2 --keep H_SampleIDs.txt --out H_AlleleFreqs
vcftools --gzvcf SSW_all_biallelic.MAF0.02.Miss0.8..recode.vcf.gz --freq2 --keep S_SampleIDs.txt --out S_AlleleFreqs
```
Now calc Fst between H and S:
```
vcftools --gzvcf SSW_all_biallelic.MAF0.02.Miss0.8..recode.vcf.gz --weir-fst-pop H_SampleIDs.txt --weir-fst-pop S_SampleIDs.txt --out HvS_Fst
```
downloading all 3 new results files onto your computer using winScp
```
fst file (HvS_Fst.weir.fst) and frequency files (H_AlleleFreqs.frq; S_AlleleFreqs.frq)
```
We have to edit the header in the frq files before we open these files in R:
```
DELETE:   {Freq}
REPLACE with:   H_REF  H_ALT
```
Make sure you do this for BOTH H and S frq files 

Now open R Studio (on desktop):

set working directory to the folder where you saved your files 

The following is the script they wanted us to copy and paste into R with a few edits I added from notes during class (can be found on the tutorial for 3-6-17):

```
# List the files in this directory -- you should see your results output from VCFTools if the download was successful
list.files()
​
# Let's do the allele freq comparisons first:
# this reads in each file; run one at a time
H_freq <- read.table("H_AlleleFreqs.frq", header=T)
S_freq <- read.table("S_AlleleFreqs.frq", header=T)
​
#look at structure of file:

str(H_freq)

# Since these files have identical numbers of SNPs in the exact same order, we can concatenate them together into one large dataframe:
All_freq <- merge(H_freq, S_freq, by=c("CHROM", "POS"))
​
# Check the results of your merge to make sure things look OK
str(All_freq) # shows the structure of the data
head(All_freq)
​
# Looks good, now let's calculate the difference in minor allele frequency at each SNP and plot as a histogram
#Since we merged the file we can now calc allele freq but subtracting the S column from the H column (see below)
All_freq$diff <- (All_freq$H_ALT - All_freq$S_ALT)
​
#now we will make a histogram to look at this data:
hist(All_freq$diff, breaks=50, col="red", main= "Allele frequency difference (H-S)")
​
# Looks like most loci show little difference (i.e., likely drift), but perhaps a few show very large differences between healthy and sick (drift or selection?)
​
# How do these highly divergent frequenices compare to Fst at the same SNPs?
fst <- read.table("HvS_Fst.weir.fst", header=T)
​
All_freq.fst <- merge(All_freq, fst, by=c("CHROM", "POS"))
​
plot(All_freq.fst$diff, All_freq.fst$WEIR_AND_COCKERHAM_FST, xlab="Allele frequency difference (H-S)", ylab="Fst", main="Healthy vs. Sick SNP divergence")
​
#in the plot the (-) individuals are sick and (+) are healthy (because we subtracted S from H)
# the "outlier" dots (top right and top left) are the ones we would be interested in looking closer at (Like in a volcano plot)

# Which are the genes that are showing the highest divergence between Healthy and Sick?
#will output the species genes; I think we get about 8 genes here
All_freq.fst[which(All_freq.fst$WEIR_AND_COCKERHAM_FST>0.2),]

#lets say we have a particular gene we want to look at:
All_freq.fst[which(All_freq.fst$CHROM=="TRINITY_DN42225_c1_g1_TRINITY_DN42225_c1_g1_i1_g.12458_m.12458"),]

#output 7 rows which means there are 7 SNPs on this particular transcript
#lets look at one particular SNP and where it likes on the plot.
#to do so we will add to the existing plot instead of making a new plot; we will do so using the x and y coordinites since we know it y=fst, x=diff, bg=background color, col=color of point, cex=modifies size of point cex=2 means twice as big)

points(0.2500000, 0.2415350, col="red", cex=5)

```



------

<div id='id-section17'/> 

### Page 17: Notes from command practice in class 2017-03-22; pop gen continued  

Todays plan:

1. start estimation of pie S and pie N
2. Compare our diversity data to Romiguier et al. 
   * estimate Ne
3. Begin investigating population structure
   * PCA
   * ADMIXTURE

Starting by looking at nucleotide diversity at syn sitess (pieS) and the ration of piN/piS)


Open putty:
```
cd /data/project_data/snps/reads2snps
head SSW_by24inds.txt.fas
tail SSW_by24inds.txt.fas
```
All the Ns mean...

The next command will take ~5hrs so we will have it running in the background:

```
screen
/data/popgen/dNdSpiNpiS_1.0 -alignment_file=SSW_by24inds.txt.fas -ingroup=sp -out=~/dNdSpiNpiS_output
```

The last command calls the program that we will use, tell it what file to use, "ingroup=sp" means we only have 1 species (no outgroup), out is what we want to save the file as

Now we need to detach from the screen
```
Ctrl+A+D
```
To get back into the the screen:
```
screen -r 
```

While we wait we can look at an example of the file that will be output (this example is from running one individual)
```
cat SSW_bamlist.txt.sum
```
Interpreting this file:

* High vaulues mean more homozygosity (vary from 0-1) 
* we have low values so it means they are probably randomly mating 
* these results also indicate we probably wont see _____(AHHHH what did he say??!?!)
* we have much less diversity of nonsyn mutations (means purifying selection)
* higher value of pin/pis ratio means selection isn't as effective at eliminating nonsynon mutations that are deleterious
* in small pops that aren't divers that have a hard time elminiating deleterious mutations
* high for vertebrates
* low for bacteria and invertibrates with huge pop sizes

Record the following for our samples:
```
piS: 0.00585312 [0.005172; 0.006598]
piN: 0.00154546 [0.00133; 0.001782]
ave. piN/piS: 0.264041 [0.223914; 0.310575]
```

Grab the following file and move to desktop:
```
/data/project_data/snps/reads2snps/Romiguier_nature13685-s3.csv
```
Open R and set working directory:
```
setwd("C:/Users/Hannah/Desktop/files for class 3-20")
```

Check the folder to make sure you are in the right place:
```
list.files()
```

now read in the romiguier file:
```
Rom <- read.csv("Romiguier_nature13685-s3.csv", header=T)
```
Check that it read in ok:
```
str(Rom) 
head(Rom)
```
*You can also click to open the table from the box in the upper right area

Now lets make a plot (only with their data for now) that shows the purifying selection vs effective pop size
```
plot(log(Rom$piS), log(Rom$piNpiS), pch=21, bg="blue", xlab="log Synonymous Nucleotide Diversity (piS)", ylab="log Ratio of Nonysn to Syn Diversity (piN/piS)", main="Purifying Selection vs. Effective Population Size")
```

you can add points to any plot by using this points command; we will add our values to the plot now:
```
points(log(0.00585312), log(0.264041), pch=24, cex=1.5, bg="red") 
```
so we added the PiS value and then the piN/piS  then told it what we wanted the point to look at

the sea stars fall in the middle(ish); maybe supprising?  not sure yet

Now we will add a best fit line; this is just using Rom data (not including ours)
```
reg <- lm(log(Rom$piNpiS) ~ log(Rom$piS)) # Fits a linear regression
abline(reg) # adds the regression line to the plot
```

It would be nice to know which data points are similar species (other echinoderms) to our sea stars 
```
echino <- Rom[which(Rom$Phylum=="Echinodermata"),] # subsets the data
points(log(echino$piS), log(echino$piNpiS), pch=21, bg="red") # adds the points
```
They are fairly spread out but there is a cluster around our sea star (this shows that selection is not doing a good job of removing deleterious mutations;low syn nucleotide diversity)
Lets add a ledgend:
```
legend("bottomleft", cex=1, legend=c("Metazoans", "Echinoderms", "P. ochraceus"), pch=c(21,21,24), col=c("blue", "red", "red"))
```

piS = theta = 4Neu

we can rearange the equation to be:

ne = piS/4u (see onenote for more details)

Now lets calc:

0.00585/(4*4*10-9)=365.625





------

<div id='id-section18'/> 

### Page 18: Notes from class commands 2017-03-27 (Self guided +notes from mtg w/Steve; PCA and DAPC   

Because I had to unzip my file within PUTTY, the class moved on without me and I got lost :(
Therefore these notes are from me trying to catch myself up
(Plus from meeting with Steve afterwards to ask questions about anything I missed)

Load libraries:

```
library(vcfR)
library(adegenet)
```

Read the vcf SNP data (the one I unzipped) into R and call it "vcf1"

```
vcf1 <- read.vcfR("SSW_all_biallelic.MAF0.02.Miss0.8.recode.vcf")
```
I got an error message saying that this file dosn't seem to exist. 

Tried again, this time instead of running the version straight up; I copied and pasted my direct file name into the command line (see below):
```
vcf1 <- read.vcfR("SSW_all_biallelic.MAF0.02.Miss0.8..recode.vcf")
```
It worked!

*I edited the script to work with my file

Since we have SOOOO many SNPS we will use one of the packages we loaded (adegenet) to store the large # of SNPS; this lets us analyze the SNPs easier

```
gl1 <- vcfR2genlight(vcf1)
print(gl1)
```
By printing the resulting file you can see that it has the right number of SNPs (5,317) and individuals (24)

We can check the file for certain information using the following commands:
```
gl1$ind.names
gl1$loc.names[1:10]
gl1$chromosome[1:3]
```
The first command (gl1$ind.names) tells you the names of each individual (they just didn't all fit on the same row so they are broken up *# in [] = # individual it starts with

The 2nd command (gl1$loc.names[1:10]) tells you the names of each loci
* Everything to the LEFT of the "_" = transcript ID
* Everything to the RIGHT of the "_" = position (ie 7456=bp location where loci (SNP) occurs within the transcript 

The 3rd command (gl1$chromosome[1:3]) shows you the transcript ID 

At this point we are adding pop to the field (actually happens at line 46 but we are building up to it)

```
ssw_meta <- read.table("ssw_healthloc.txt", header=T) # read in the metadata
ssw_meta <- ssw_meta[order(ssw_meta$Individual),] # sort by Individual ID, just like the VCF file
```

The first command read in the meta data and called it "ssw_meta"

The second command organized the meta data so that it was sorted by individual ID

We want to confirm that the IDs are ordered the same in both the SNP data file (the we read in earlier and called "gl1") and the meta data file (that we just read in and called "ssw_meta"

When we match data from ssw_meta to gl1 they just read it in in the order that it is in.  That is why we are making sure that the order is the same for each file so that the data will match up correctly

```
gl1$ind.names
ssw_meta$Individual
```
Here is what it looks like after running those commands:
```
> gl1$ind.names
 [1] "03" "07" "08" "09" "10" "14" "15" "19"
 [9] "20" "22" "23" "24" "26" "27" "28" "29"
[17] "31" "32" "33" "34" "35" "36" "37" "38"
> ssw_meta$Individual
 [1]  3  7  8  9 10 14 15 19 20 22 23 24 26 27
[15] 28 29 31 32 33 34 35 36 37 38
```
They don't have to match exactly (07 and 7 is ok in this case)
The # in the brackets is only indicating where the line picks up (for example [15] means it picks up with ind #15 (not ind15); it is NOT the row #

Now we will assign locality (first line of code below) and disease status (second line of code below)
* when we say "assign" locality etc we mean we will use tell the gl1 file what the location and disease status of each individual is
* basically we are merging the files to have one file with all the info we need to make the comparisons and calculations we will do later today.
```
gl1$pop <- ssw_meta$Locality 
gl1$other <- as.list(ssw_meta)
```
We can look at the SNP data by generating the following plot:
```
glPlot(gl1, posi="bottomleft")
```
Notes on the plot:
* white space = missing data
* Number of 2nd allele key explained:
* 0= AA
* 1= AT
* 2= TT
* x-axis: each column is a different SNP (there are 5000+ columns)
* y-axis: each row is a different individual 

If you want to know which individuals have missing data (white) use the following command to have it tell you what individual is at which row (right now the y axis list the individuals but dosn't correlate with the ind ID (ie row 2 is actually individual 7)
```
gl1$ind.names[2]
```

too look up more then one at a time:

```
gl1$ind.names[c(2, 10)]
```

Now we will calculate THEN plot the PCA (principle coordinate analysis)
*NOTE: "pca1" does NOT equal principle componant 1 (its just what we are calling the first pca we are running)

1. Calculate PCA(1) (line 41)
```
pca1 <- glPca(gl1, nf=4, parallel = F)
pca1 
```
nf =  # of principle componants to fit in one model
second command prints summary

"$" in summary (line 42) show you the lines you can look at (ie scores, loadings etc)
if you want to look at specific categories you can run the following command:

```
pca1$scores
```

2. Plot PCA(1) (line 45)
```
plot(pca1$scores[,1], pca1$scores[,2], 
     cex=2, pch=20, col=gl1$pop, 
     xlab="Principal Component 1", 
     ylab="Principal Component 2", 
     main="PCA on SSW data (Freq missing=20%; 5317 SNPs)")
legend("topleft", 
       legend=unique(gl1$pop), 
       pch=20, 
       col=c("black", "red"))
```

The above command:
* plots the principle component calcualtions performed by running line 41
  * line 41 calculated PC for ALL SNPs (not just ones specific to health or location, etc)
  * "col=gl1$pop" is the specific part that tells it to color the dots based on location
  * NOTE: by telling it to color them by location it does NOT change where the points are; it mearly colors them based on extra info.  The dots are ploted based on PC which was calculated for SNPs and did NOT take these extra variable into acount

Detailed explination of what means what in the command we ran (line 45-53)
```
plot(pca1$scores[,1], pca1$scores[,2], #Calc 4 but only looking at 1 and 2
     cex=2, #point/dot size; will assume cex=1 if you don't specify
	 pch=20 #type of symbol (20=filled circle; can seach for other codes via google or "?pch")
	 col=gl1$pop, #Tell it what to look at (col=color, we said to look at location (which we assigned to "pop")
     xlab="Principal Component 1", #title for x-axis 
     ylab="Principal Component 2", #title for y-axis 
     main="PCA on SSW data (Freq missing=20%; 5317 SNPs)") #title
legend("topleft", #where to put legend 
       legend=unique(gl1$pop), #if you don't specify "unique" it will list allllll the int and subs (ie int int int sub sub int..); "legend=unique" tells it list each diff variable once (int, sub)
       pch=20, 
       col=c("black", "red")) #telling it to match the colors used in the plot
```

Extra explination of PCA plots (in general):
* the data points in the (+) associate more with the alternative allele
* data points in the (-) associate more with the reference allele (which was determined during the transcriptome assembly (using trinity))
* Each data point represents the AVERAGE of the SNPs per individual (aka its not showing us which SNP associate with which allele; it shows us OVERALL more SNPs associate with the alt/ref allele)


Now we will change the color of the dots to show health staus
* NOTE: remember this will NOT change the data itself but will mearly show you the health status vs location associate with each individual 

```
plot(pca1$scores[,1], pca1$scores[,2], 
     cex=2, pch=20, col=gl1$other$Trajectory, 
     xlab="Principal Component 1", 
     ylab="Principal Component 2", 
     main="PCA on SSW data (Freq missing=20%; 5317 SNPs)")
legend("topleft", 
       legend=unique(gl1$other$Trajectory), 
       pch=20, 
       col=unique(gl1$other$Trajectory))
```

Now that we have out PCA data we can ask more specific questions like:

*Which SNPs load most strongly on the 1st PC axis?*
```
loadingplot(abs(pca1$loadings[,1]),
            threshold=quantile(abs(pca1$loadings), 0.999))
```
Some notes about the command you just ran above:
* 0.999 = 99.9 percentile (it set the threshold (the horizontal line in the figure) to "cut off" and identify SNPs that were beyond that percentile (?right?)
  * you can try running it at 0.95 (95th percentile) and it will identify MANY more SNPs; this is because it is a less stringent cut off
  * with SNPs since we have so many you need a super stringent cut off (which is why we use 0.999; acceptable = <1%)

If we want to identify (name those SNPs that were above the threshold we can run the following command:
```
gl1$loc.names[which(abs(pca1$loadings)>quantile(abs(pca1$loadings), 0.999))]
```

to look up the actual loading value associated with a particular SNP (for example SNP#3412) we do the following:

```
pca1$loadings[3412]
```
it spat out:
```
[1] 0.1397889
```



************NOW WE MOVE ONTO THE DAPC METHODS************

DAPC (Discriminant analysis of principle componants)
* Tries to decide which SNPs *attributed* to the health status	
  * not grouping "these SNPs were found in indiv with HH or HS...
  * instead we are trying to find that SNPs that might have caused/that attribute to the health status 
  * getting more specific then PCA

*The below command does the dapc and will re-run the PCA calc:
```
disease.dapc <- dapc(gl1, pop=gl1$other$Trajectory, n.pca=8, n.da=3,
                     var.loadings=T, pca.info=T, parallel=F)
```

Detailed expliation of each part of the above command:
```
disease.dapc <- dapc(gl1,
				pop=gl1$other$Trajectory, #pop = what you are grouping it by (here we are grouping it by disease)
				n.pca=8, #how many pcs to use; it will calculate 8 here); rule of thumb: don't want to use n/3 pcs (ie for us don't use mroe then # of ind/3; 24/3=8)
				n.da=3, # number of discriminant functions it is using; we have 4 health statuses and you should be able to describe anything with the #variables-1 (ie for us 4-1=3)
                var.loadings=T, # means we want to look at them so output them
				pca.info=T, #we want to beable to ask info abt pc axis in the output 
				parallel=F) #if you need more cores you tell it to use more by making "parallel=T"; here we tell it to use only one because this command can't seem to use more then one 
```

Look at summary of dapc we just ran by running the following:
```
disease.dapc
```
*loadings = info for each pc we calc (should be 8)
*PC.loadings = info for all the SNPs 

Now we will take the dapc data and plot it; we will use a scatter plot:
```
scatter.dapc(disease.dapc, grp=gl1$other$Trajectory, legend=T)
```
shows all three DA (Discriminant analyses) in the bottom right bar graph style shaded box but it ploted only 1 and 2 (the horizontal and vertical lines that form a "+" in your figure)

We can also show the data as a stacked bar graph
* this helps us to see the probablity for each health status that dacp calculated for each individaul

```
compoplot(disease.dapc)
```

You can use the above plot to compare to the scatter plot generated just before (line 84) to help identify individuals who are linked as (for example) HS but their dot is very close to HH 

Finally (for today) you can generate another loading plot to see which SNPs contribute most to distiguishing H vs S individuals 

```
loadingplot(abs(disease.dapc$var.load), 
            lab.jitter=1, 
            threshold=quantile(abs(disease.dapc$var.load), probs=0.999))
```
------

<div id='id-section19'/> 

### Page 19: Notes from class commands 2017-03-29; ADMIXTURE  

Admixture analysis:   

Today:   
1) PCA/DAPC Questions?   
2) Admixture    
* Theory   
* analysis   
* plotting   
  3) Homework #3 discussion   

Main goal:   
* See onenote for detailed explination    

We will need 4 files:   
* the input data file in vcf format   
* a text file with sample IDs and the population designations   
* a settings file (.spid) that tells PGDSpider how to process the data   
* a bash script that runs the program with all the above settings specified   

More specifically:   

```   
/data/project_data/snps/reads2snps/SSW_tidal.pops   
/data/project_data/snps/reads2snps/vcf2admixture_SSW.spid   
/data/project_data/snps/reads2snps/vcf2geno.sh   
```

Look at the SSW_tidal.pops file   

```   
cat SSW_tidal.pops   
```

look at the vcf2admixture_SSW.spid   

```   
vim vcf2admixture_SSW.spid   
```

The writer_format is the format it will output the file as.  It is set to what we want    

Next we want to copy to the home directory these files:   

```   
cp SSW_tidal.pops ~/   
cp vcf2admixture_SSW.spid ~/   
cp vcf2geno.sh ~/    
```

Go check that they are there and that they are unzipped:   

```   
cd ~/   
```

All there and already unzipped =)   

Use vim to open up the vcf2geno.sh   

```   
vim vcf2geno.sh    
```

a bash script lets you write commands like you would in a command line but it lets you store them.   

It will run then in order one at a time     

It has 2 functions:   
1) reproducable science (helps you remember and share what you did)   
2) you can have it run in the background while you do other things   

EVERY single bash script needs this line:   

```   
#!/bin/bash   
```

Next it tells you what language its in (java)   
then it says where your file is    
"-jar" name of the program you want to run    
"-inputfile" is what you will rename YOUR file as    
"-inputformat" what type of file it is (VCF for us)   
"-outputfile" what your output file will be called; Steve recommends that you name it the same as your info except it will have the ".geon" at the end   
"-outputformat" what format you want the output in (we want EIGENSOFT)   
"-spid" telling it to use the spid file because that is where you put all the settings you want to use    

Now edit the file to match your file names    

```   
i #allows you to edit    
# change the inputfile name   
# change the output file name    
# when done hit "esc" then   
:wq #to save and quit   
```

Once done you can run the file:   

```   
bash vcf2geno.sh   
```

Once done running you can look at the output file:   

```   
vim SSW_all_biallelic.MAF0.02.Miss0.8..recode.vcf.geno    
```

It will show a bunch of numbers (0, 1, 2, 9) which is what we expect    

It tells the computer how to interpret the lines that follow   

Now we have the input file we need to run admixture   

we want to run it for a variety of K values    
* we get to choose how many Ks we want to look at    
* K of 1-10 is a good place to start (5-10 is pretty good)   

Since we want to run a few we have a script to use; copy that script to our home directory:   

```   
cd /data/project_data/snps/reads2snps   
ll    
cp ADMIX.sh ~/   
```

Make sure its there then open the file:   

```   
cd ~/   
ll   
vim ADMIX.sh   
```

Breaking this down:   

```   
admixture -C 0.000001 --cv ./SSW_all_biallelic.MAF0.02.Miss1.0.recode.vcf.geno $K \   
| tee log${K}.out   
```

-C (see onenote for more detail) it is the point where it reaches the maximum likelihood   
tee: take everything you would have printed to screen and send it to this file instead    

```   
grep CV log*.out >chooseK.txt   
```

use grep to search within each file to a chooseK.txt file (lets us focus on the valuess of K that minimize the cross validation)   

Now run the command:   

```   
bash ADMIX.sh    
```

Once its done running you can open the "chooseK.txt" file   

```   
cat choosek.txt    
```
We can tell that K=1 is the best model because is had the lowest value of error rate (the value listed ater (k=10): ____)   
Second best is K=10 which is kind of interesting...to try to figure out why we ran it again but we are now looking at K from 1-20 (instead of 1-10)   



Homework:   
take tools you learned and re-apply them with different filtering strategies    

We defined the following filtering straties in class:   
--min-alleles 2   
--max-alleles 2   
--max-missing 0.8   
--recode   
--out filename   

------

<div id='id-section20'/> 

### Page 20: Notes from class commands 2017-04-03; OutFLANK  

We moved these files from the server to the folder on the desktop that we will use as our working directory in R:   

* SSW_all_biallelic.MAF0.02.Miss0.8.recode.vcf.geno       
* SSW_all_biallelic.MAF0.02.Miss0.8..recode.vcf      
* SSW_healthloc.txt        

Go into R and set your working directory   

Then install these packages:   

```   
install.packages("devtools")   
install_github("whitlock/OutFLANK")   
```

Once installed open thse packages plust the following:   
```   
library(OutFlank)   
library(vcfR)   
library(adegenet)   
```
NOTE: Choose to update **all** when asked (for "whitlock/OutFLANK")   


Example code for ____   
```   
OutFLANK(FstDataFrame, LeftTrimFraction = 0.05, RightTrimFraction = 0.05,
  Hmin = 0.1, NumberOfSamples, qthreshold = 0.05)
```

Explination of what each part means according to R    

```
FstDataFrame	
A data frame that includes a row for each locus, with columns as follows:
$LocusName: a character string that uniquely names each locus.
$FST: Fst calculated for this locus. (Kept here to report the unbased Fst of the results)
$T1: The numerator of the estimator for Fst (necessary, with $T2, to calculate mean Fst)
$T2: The denominator of the estimator of Fst
$FSTNoCorr: Fst calculated for this locus without sample size correction. (Used to find outliers)
$T1NoCorr: The numerator of the estimator for Fst without sample size correction (necessary, with $T2, to calculate mean Fst)
$T2NoCorr: The denominator of the estimator of Fst without sample size correction
$He: The heterozygosity of the locus (used to screen out low heterozygosity loci that have a different distribution)
$indexOrder: integer index giving the original order of rows in the input file.
LeftTrimFraction	
The proportion of loci that are trimmed from the lower end of the range of Fst before the likelihood funciton is applied.
RightTrimFraction	
The proportion of loci that are trimmed from the upper end of the range of Fst before the likelihood funciton is applied.
Hmin	
The minimum heterozygosity required before including calculations from a locus.
NumberOfSamples	
The number of spatial locations included in the data set. **FOR US IT WILL BE THREE; HH HS SS**
qthreshold	
The desired false discovery rate threshold for calculating q-values.
```


**This class ran fast and we encountered many errors along the way.  I will paste the completed updated Rscript below (that one that SHOULD work without errors)


```
setwd("C:/Users/Hannah/Desktop/DGE data from 2-27")

# Install Packages
install.packages("devtools")
library(devtools)
source("http://bioconductor.org/biocLite.R")
biocLite("qvalue")
install_github("whitlock/OutFLANK")

#Load these packages
library(OutFLANK)
library(vcfR)
library(adegenet)

#Read in your .geno file. OutFlank requires it to be transposed, so we'll so that next. 
ssw.geno_in <- read.fwf("SSW_all_biallelic.MAF0.02.Miss0.8.recode.vcf.geno", width=rep(1,24))

#transposing
ssw.geno <- t(ssw.geno_in)

#read in the meta data
ssw_meta <- read.table("ssw_healthloc.txt",T)  #read in the data
ssw_meta <- ssw_meta[order(ssw_meta$Individual),] #reorder the meta_data by individual number
ssw_meta$Trajectory[which(ssw_meta$Trajectory == 'MM')] = NA #Remove the MM's from the analysis (takes the MM and sets it as NA)

#Now we can use OutFLANK
OF_SNPs <- MakeDiploidFSTMat(ssw.geno, locusNames = seq(1,5317,1), popNames = ssw_meta$Trajectory)
OF_out <- OutFLANK(FstDataFrame = OF_SNPs, LeftTrimFraction = 0.05, RightTrimFraction = 0.05, Hmin = 0.1, NumberOfSamples = 3, qthreshold = 0.1)
OutFLANKResultsPlotter(OF_out, withOutliers = T, NoCorr = T, Hmin = 0.1, binwidth = 0.005, titletext = "Scan for local selection")
outliers <- which(OF_out$results$OutlierFlag=="TRUE")
outliers 

#We can extract info about the  outliers by reading in the vcf files and looking at the annotations 
vcf1 <- read.vcfR("SSW_all_biallelic.MAF0.02.Miss0.8..recode.vcf") #Read in file
vcfann <- as.data.frame(getFIX(vcf1)) #Extract annotations from the file
vcfann[outliers,]
```

For outliers you should have gotten:   

```
                                                            CHROM  POS   ID REF ALT QUAL FILTER
1223 TRINITY_DN46509_c0_g1_TRINITY_DN46509_c0_g1_i1_g.22498_m.22498 1036 <NA>   G   A <NA>   PASS
1452 TRINITY_DN46269_c0_g1_TRINITY_DN46269_c0_g1_i1_g.21774_m.21774 1242 <NA>   C   T <NA>   PASS
3067 TRINITY_DN46834_c0_g2_TRINITY_DN46834_c0_g2_i3_g.23511_m.23511  408 <NA>   A   G <NA>   PASS
3068 TRINITY_DN46834_c0_g2_TRINITY_DN46834_c0_g2_i3_g.23511_m.23511  411 <NA>   G   A <NA>   PASS
3546 TRINITY_DN43783_c3_g1_TRINITY_DN43783_c3_g1_i6_g.15491_m.15491  110 <NA>   T   C <NA>   PASS
3602 TRINITY_DN42222_c4_g2_TRINITY_DN42222_c4_g2_i1_g.12455_m.12455   61 <NA>   C   T <NA>   PASS
4374 TRINITY_DN44735_c6_g1_TRINITY_DN44735_c6_g1_i3_g.17655_m.17655   80 <NA>   C   G <NA>   PASS
```


Now we will go into **Putty** to look at the VCF file, grad the nucleotide line for the outliers we just discovered, and BLAST them    

```
cd ~/
vim SSW_all_biallelic.MAF0.02.Miss0.8..recode.vcf
```

If you only have one vcf file you can tell it to open anything with .vcf by running the following command   

```
vim *.vcf
```

Now you want to set no wrap by doing the following:   

```
(SHIFT) :set nowrap (ENTER)
```

WRONG!  Don't use the vcf file!  so quit out and go to the correct file    

```
:q!
```

Now got to file and vim (open it)    

```
cd /data/project_data/assembly/
ll
vim 08-11-35-36_cl20_longest_orfs_gene.cds
```

When in the file do the following commands:   
```
(SHIFT) :set nowrap
/(name of outlier you are searching for)
```

Once it finds the outlier copy the line of nucleotides and BLAST it (in BLASTx)   

USE BLASTx; can also se BLASTp    
NOTE: some of these will not align to anything    

------

<div id='id-section21'/> 

### Page 21: Script for Homework 3; vcftools and PCA plots  



### VCFtools code:

Code from 3-31-17

```
cd /data/project_data/snps/reads2snps
```
we want the latest version of this file:

```
SSW_by24inds.txt.vcf.gz
```

Its zipped so we need to tell it that we are going to read in a zipped vcf file when we put "--gzvcf"

```
vcftools --gzvcf SSW_by24inds.txt.vcf.gz
```

shows that there are 24 individuals and 7,486,938 SNPs

Now is when I will start playing with the filters.  I want to keep eye on how each filter changes the SNP# so I will test one filter at time (keeping all the other filters set to what we had them at in class).  
I will start by testing the **'min-alleles'** filter

```
vcftools --gzvcf SSW_by24inds.txt.vcf.gz --min-alleles 1 --max-alleles 2 --maf 0.02 --max-missing 0.8 --recode --out ~/SSW_all_biallelic.mina1.MAF0.02.Miss0.8_HW3 
```

After changing the 'min-alleles 1' we have 5,317 SNPs left (Thats 7,481,621 SNPs that were cut)

Lets see if the same thing happens when I change the **'max-alleles'** to 1

```
vcftools --gzvcf SSW_by24inds.txt.vcf.gz --max-alleles 1 --maf 0.02 --max-missing 0.8 --recode --out ~/SSW_all_biallelic.maxa1.MAF0.02.Miss0.8_HW3 
```

HA! Now there are 0 SNPs that are left

Moving on, lets just try setting the max to a higher number 

```
vcftools --gzvcf SSW_by24inds.txt.vcf.gz --max-alleles 4 --maf 0.02 --max-missing 0.8 --recode --out ~/SSW_all_biallelic.maxa4.MAF0.02.Miss0.8_HW3 
```

There are 5366  SNPs left (so similar to the min-alleles 1 filter)

The previous command did NOT have a min-alleles filter so I'll try it again with 'min-alleles 2'

```
vcftools --gzvcf SSW_by24inds.txt.vcf.gz --min-alleles 2 --max-alleles 4 --maf 0.02 --max-missing 0.8 --recode --out ~/SSW_all_biallelic.maxa4.MAF0.02.Miss0.8_HW3 
```

Adding the 'min-alleles 2' filter did not change the number of SNPs that were kept (5366) (at least it didn't not change things this time)

To wrap up looking at min and max allele filters we will run the original version of the command which has min and max set to 2.

```
vcftools --gzvcf SSW_by24inds.txt.vcf.gz --min-alleles 2 --max-alleles 2 --maf 0.02 --max-missing 0.8 --recode --out ~/SSW_all_biallelic.MAF0.02.Miss0.8_HW3 
```
The original filters kept 5,317 SNPs (same as when min-alleles was set to 1)



OK moving on to test different filters 

Next we will test the **'maf'** (minor allele frequency) filter
According to the vcf manual *Allele frequency* is defined as the number of times an allele appears over all individuals at that site, divided by the total number of non-missing alleles at that site.
a typical maf filter is 0.01; the higher end of this filter is around 0.05
few loci get to be "common" 
* Setting a high threshold for this filter allows you to only look at common SNPs 

I will try a lower (0.01) higher (0.05) filter and see what happens

```
vcftools --gzvcf SSW_by24inds.txt.vcf.gz --min-alleles 2 --max-alleles 2 --maf 0.01 --max-missing 0.8 --recode --out ~/SSW_all_biallelic.MAF0.01.Miss0.8_HW3 
```

It kept 5,317 SNPs (same as class set filter)

Now lets try the higher threshold:

```
vcftools --gzvcf SSW_by24inds.txt.vcf.gz --min-alleles 2 --max-alleles 2 --maf 0.05 --max-missing 0.8 --recode --out ~/SSW_all_biallelic.MAF0.05.Miss0.8_HW3 
```

The 0.05 threshold retained only 1,899 SNPs 

NOW lets try the **'max-missing'** filter:
According to the VCFtools manual this filter: "Exclude sites on the basis of the proportion of missing data (defined to be between 0 and 1, where 0 allows sites that are completely missing and 1 indicates no missing data allowed)."

First we can try seeing what it would be like if we were very lax (aka if we set max-missing to 0 which allows for missing data):

```
vcftools --gzvcf SSW_by24inds.txt.vcf.gz --min-alleles 2 --max-alleles 2 --maf 0.02 --max-missing 0 --recode --out ~/SSW_all_biallelic.MAF0.02.Miss0_HW3 
```

This filter kept 56,120 SNPs

Now lets try the opposite extream; NO missing data allowed

max-missing 1

```
vcftools --gzvcf SSW_by24inds.txt.vcf.gz --min-alleles 2 --max-alleles 2 --maf 0.02 --max-missing 1 --recode --out ~/SSW_all_biallelic.MAF0.02.Miss1_HW3 
```

Now we only have 1,494 SNPs left 

Lets see if we can find a good inbetween for this filter.  We will start with 0.5

```
vcftools --gzvcf SSW_by24inds.txt.vcf.gz --min-alleles 2 --max-alleles 2 --maf 0.02 --max-missing 0.5 --recode --out ~/SSW_all_biallelic.MAF0.02.Miss0.5_HW3 
```

With this filter we ended up with 15,301 SNPs


***At this point I took a break and came back later to test more filters***


Now I will increase the max-missing filter by 0.1 to see what happens at each level. 

max-missing 0.6

```
vcftools --gzvcf SSW_by24inds.txt.vcf.gz --min-alleles 2 --max-alleles 2 --maf 0.02 --max-missing 0.6 --recode --out ~/SSW_all_biallelic.MAF0.02.Miss0.6_HW3
```

10,906 SNPs

**************Paused here; picked up below on 4/3****************

First we have to get back to the correct location to run commands:   

```
cd /data/project_data/snps/reads2snps
```
Now we can contiue our max missing gradient:

Max-missing 0.7

```
vcftools --gzvcf SSW_by24inds.txt.vcf.gz --min-alleles 2 --max-alleles 2 --maf 0.02 --max-missing 0.7 --recode --out ~/SSW_all_biallelic.MAF0.02.Miss0.7_HW3
```

8452 SNPs

max-missing 0.9 (0.8 was used in class) 

```
vcftools --gzvcf SSW_by24inds.txt.vcf.gz --min-alleles 2 --max-alleles 2 --maf 0.02 --max-missing 0.9 --recode --out ~/SSW_all_biallelic.MAF0.02.Miss0.9_HW3
```

3371  SNPs


NEXT FILTER: we will test the "minDP" filter which looks at read depth
* low value 5
* high value >100; Start having reads from paralogus genomes matching to copies. 

We will start with the low threshold and work our way up to see what happens 

minDP = 5

```
vcftools --gzvcf SSW_by24inds.txt.vcf.gz --min-alleles 2 --max-alleles 2 --maf 0.02 --max-missing 0.8 --minDP 5 --recode --out ~/SSW_all_biallelic.MAF0.02.Miss0.8minDP5_HW3
```

5317  SNPs

minDP 50

```
vcftools --gzvcf SSW_by24inds.txt.vcf.gz --min-alleles 2 --max-alleles 2 --maf 0.02 --max-missing 0.8 --minDP 50 --recode --out ~/SSW_all_biallelic.MAF0.02.Miss0.8minDP50_HW3
```

5317  SNPs


minDP 100

```
vcftools --gzvcf SSW_by24inds.txt.vcf.gz --min-alleles 2 --max-alleles 2 --maf 0.02 --max-missing 0.8 --minDP 100 --recode --out ~/SSW_all_biallelic.MAF0.02.Miss0.8minDP5100_HW3
```

5317 SNPs


minDP 200

```
vcftools --gzvcf SSW_by24inds.txt.vcf.gz --min-alleles 2 --max-alleles 2 --maf 0.02 --max-missing 0.8 --minDP 200 --recode --out ~/SSW_all_biallelic.MAF0.02.Miss0.8minDP200_HW3
```

5317 SNPs

....Interesting.  There was no change in the number of SNPs called  for any of these filters.  Either I did something wrong or there was no effect of minDP on our dataset


NEXT FILTER: hwe
* This tests removes data that deviated from the random mating equilibrium
  * i.e. hwe 0.01 rejects any SNPs that violate the random mating equilibrium at 0.01

We will start with 0.01 and go from there 

```
vcftools --gzvcf SSW_by24inds.txt.vcf.gz --min-alleles 2 --max-alleles 2 --maf 0.02 --max-missing 0.8 --hwe 0.01 --recode --out ~/SSW_all_biallelic.MAF0.02.Miss0.8minhwe0.01_HW3
```

5301  SNPs 
Alright so this threshold didn't change the # of SNPs too drastically so I'll try values lower and higher then this one:

hwe 0.001

```
vcftools --gzvcf SSW_by24inds.txt.vcf.gz --min-alleles 2 --max-alleles 2 --maf 0.02 --max-missing 0.8 --hwe 0.001 --recode --out ~/SSW_all_biallelic.MAF0.02.Miss0.8minhwe0.001_HW3
```

5311  SNPs 


hwe 0.1

```
vcftools --gzvcf SSW_by24inds.txt.vcf.gz --min-alleles 2 --max-alleles 2 --maf 0.02 --max-missing 0.8 --hwe 0.1 --recode --out ~/SSW_all_biallelic.MAF0.02.Miss0.8minhwe0.1_HW3
```

5161   SNPs 


Summary:

- Min-alleles
  - 1 (5,317 SNPs)
  - 2 (5,317 SNPs; filter used in class)

- Max-alleles
  - 1 (0 SNPs)
  - 2 (5,317 SNPs; filter used in class)
  - 4 (5,366 SNPs)

- maf 
  - 0.1 (5,317 SNPs)
  - 0.5 (1,899 SNPs)
  - 0.2 (5,317 SNPs; filter used in class)

- max-missing 
  - 0 (56,120 SNPs)
  - 1 (1,494 SNPs)
  - 0.5 (15,301 SNPs)
  - 0.6 (10,906 SNPs)
  - 0.7 (8,452 SNPs)
  - 0.8 (5,317 SNPs; filter used in class)
  - 0.9 (3,371 SNPs)

- minDP
  - 5 (5,317 SNPs)
  - 50 (5,317 SNPs)
  - 100 (5,317 SNPs)
  - 200 (5,317 SNPs)

- hwe 
  - 0.01 (5,301 SNPs)
  - 0.001 (5,311 SNPs)
  - 0.1 (5,161 SNPs)


  ​
### BELOW STARTS THE SCRIPT FOR PCA PLOT GENERATION

```
#Script from class modified for homework 3
# Set your working directory to where you downloaded your results files:
#setwd("~/github/PBIO381_srkeller_labnotebook/data/SNP_data/")

list.files() # Do you see your downloaded files there? If not, double check to make sure you've set your working directory to the right spot

# We'll need to install 2 packages to work with the SNP data:
#install.packages("vcfR") # reads in vcf files and proides tools for file conversion 
#install.packages("adegenet") # pop-genetics package with some handy routines, including PCA and other multivariate methods (DAPC)

# ...and load the libraries
library(adegenet)
library(vcfR)

#Read the vcf SNP data into R
download.file("https://raw.githubusercontent.com/stephenrkeller/PBIO381_srkeller_labnotebook/master/data/SNP_data/SSW_all_biallelic.MAF0.02.Miss0.8.recode.vcf",dest="test.vcf")

vcf1<-read.vcfR("test.vcf")
vcf1 <- read.vcfR("SSW_all_biallelic.MAF0.02.Miss0.8..recode.vcf")

# The adegenet package uses a highly efficient way of storing large SNP datasets in R called a "genlight" object. The following function creates a genlight object from your vcf:
gl1 <- vcfR2genlight(vcf1)
print(gl1) # Looks good! Right # of SNPs and individuals!

# For info, try:
gl1$ind.names
gl1$loc.names[1:10]

# Notice there's nothing in the field that says "pop"? Let's fix that...
ssw_meta <- read.table("ssw_healthloc.txt", header=T) # read in the metadata
ssw_meta <- ssw_meta[order(ssw_meta$Individual),] # sort it by Individual ID

# Confirm the ID's are ordered the same in gl1 and ssw_meta:
gl1$ind.names
ssw_meta$Individual

gl1$pop <- ssw_meta$Location # assign locality info
gl1$other <- as.list(ssw_meta$Trajectory) # assign disease status


# WE can explore the structure of our SNP data using the glPlot function, which gives us a sample x SNP view of the VCF file
glPlot(gl1, posi="bottomleft")

# Now, let's compute the PCA on the SNP genotypes and plot it:
pca1 <- glPca(gl1, nf=4, parallel = F)
pca1 

# Plot the individuals in SNP-PCA space, with locality labels:
plot(pca1$scores[,1], pca1$scores[,2], 
     cex=2, pch=20, col=gl1$pop, 
     xlab="Principal Component 1", 
     ylab="Principal Component 2", 
     main="PCA on SSW data (max-missing 0.8; maf 0.02)")
legend("topleft", 
       legend=unique(gl1$pop), 
       pch=20, 
       col=c("black", "red"))

# Perhaps we want to show disease status instead of locality:
plot(pca1$scores[,1], pca1$scores[,2], 
     cex=2, pch=20, col=as.factor(unlist(gl1$other)), 
     xlab="Principal Component 1", 
     ylab="Principal Component 2", 
     main="PCA on SSW data (Freq missing=20%; 5317 SNPs)")
legend("topleft", 
       legend=unique(as.factor(unlist(gl1$other))), 
       pch=20, 
       col=as.factor(unique(unlist(gl1$other))))

# Which SNPs load most strongly on the 1st PC axis?
loadingplot(abs(pca1$loadings[,1]),
            threshold=quantile(abs(pca1$loadings), 0.999))
# Get their locus names
gl1$loc.names[which(quantile(abs(pca1$loadings))>0.999)]

threshold<-quantile(abs(pca1$loadings),0.999)

gl1$loc.names[which(abs(pca1$loadings)>threshold)]

gl1$loc.names[which(quantile(abs(pca1$loadings),0.999)>0.0770)]



#Below starts the analysis for the two vcf files I generated for homework
#Start with PCA2 = max missing 0.9

vcf2 <- read.vcfR("SSW_all_biallelic.MAF0.02.Miss0.9_HW3.recode.vcf")

# The adegenet package uses a highly efficient way of storing large SNP datasets in R called a "genlight" object. The following function creates a genlight object from your vcf:
gl2 <- vcfR2genlight(vcf2)
print(gl2) # Looks good! Right # of SNPs and individuals!

# For info, try:
#gl1$ind.names
#gl1$loc.names[1:10]

# Notice there's nothing in the field that says "pop"? Let's fix that...
#ssw_meta <- read.table("ssw_healthloc.txt", header=T) # read in the metadata
#ssw_meta <- ssw_meta[order(ssw_meta$Individual),] # sort it by Individual ID

# Confirm the ID's are ordered the same in gl1 and ssw_meta:
gl2$ind.names
ssw_meta$Individual

gl2$pop <- ssw_meta$Location # assign locality info
gl2$other <- as.list(ssw_meta$Trajectory) # assign disease status


# WE can explore the structure of our SNP data using the glPlot function, which gives us a sample x SNP view of the VCF file
glPlot(gl2, posi="bottomleft")

# Now, let's compute the PCA on the SNP genotypes and plot it:
pca2 <- glPca(gl2, nf=4, parallel = F)
pca2 

# Plot the individuals in SNP-PCA space, with locality labels:
plot(pca2$scores[,1], pca2$scores[,2], 
     cex=2, pch=20, col=gl2$pop, 
     xlab="Principal Component 1", 
     ylab="Principal Component 2", 
     main="PCA on SSW data (max-missing 0.9; maf 0.02)")
legend("topleft", 
       legend=unique(gl1$pop), 
       pch=20, 
       col=c("black", "red"))

# Perhaps we want to show disease status instead of locality:
plot(pca2$scores[,1], pca2$scores[,2], 
     cex=2, pch=20, col=as.factor(unlist(gl2$other)), 
     xlab="Principal Component 1", 
     ylab="Principal Component 2", 
     main="PCA on SSW data (max-missing 0.9; maf 0.02; 5317 SNPs)")
legend("topleft", 
       legend=unique(as.factor(unlist(gl2$other))), 
       pch=20, 
       col=as.factor(unique(unlist(gl2$other))))

# Which SNPs load most strongly on the 1st PC axis?
loadingplot(abs(pca2$loadings[,1]),
            threshold=quantile(abs(pca2$loadings), 0.999))
# Get their locus names
gl2$loc.names[which(quantile(abs(pca2$loadings))>0.999)]

threshold<-quantile(abs(pca2$loadings),0.999)

gl2$loc.names[which(abs(pca2$loadings)>threshold)]

gl2$loc.names[which(quantile(abs(pca2$loadings),0.999)>0.0770)]



#running the third and final PCA
#PCA3 = maf 0.05


vcf3 <- read.vcfR("SSW_all_biallelic.MAF0.05.Miss0.8_HW3.recode.vcf")

# The adegenet package uses a highly efficient way of storing large SNP datasets in R called a "genlight" object. The following function creates a genlight object from your vcf:
gl3 <- vcfR2genlight(vcf3)
print(gl3) # Looks good! Right # of SNPs and individuals!

# For info, try:
#gl1$ind.names
#gl1$loc.names[1:10]

# Notice there's nothing in the field that says "pop"? Let's fix that...
#ssw_meta <- read.table("ssw_healthloc.txt", header=T) # read in the metadata
#ssw_meta <- ssw_meta[order(ssw_meta$Individual),] # sort it by Individual ID

# Confirm the ID's are ordered the same in gl1 and ssw_meta:
gl3$ind.names
ssw_meta$Individual

gl3$pop <- ssw_meta$Location # assign locality info
gl3$other <- as.list(ssw_meta$Trajectory) # assign disease status


# WE can explore the structure of our SNP data using the glPlot function, which gives us a sample x SNP view of the VCF file
glPlot(gl3, posi="bottomleft")

# Now, let's compute the PCA on the SNP genotypes and plot it:
pca3 <- glPca(gl3, nf=4, parallel = F)
pca3 

# Plot the individuals in SNP-PCA space, with locality labels:
plot(pca3$scores[,1], pca3$scores[,2], 
     cex=2, pch=20, col=gl3$pop, 
     xlab="Principal Component 1", 
     ylab="Principal Component 2", 
     main="PCA on SSW data (max-missing 0.8; maf 0.05)")
legend("topleft", 
       legend=unique(gl1$pop), 
       pch=20, 
       col=c("black", "red"))

# Perhaps we want to show disease status instead of locality:
plot(pca3$scores[,1], pca3$scores[,2], 
     cex=2, pch=20, col=as.factor(unlist(gl3$other)), 
     xlab="Principal Component 1", 
     ylab="Principal Component 2", 
     main="PCA on SSW data (Freq missing=20%; 5317 SNPs)")
legend("topleft", 
       legend=unique(as.factor(unlist(gl3$other))), 
       pch=20, 
       col=as.factor(unique(unlist(gl3$other))))

# Which SNPs load most strongly on the 1st PC axis?
loadingplot(abs(pca3$loadings[,1]),
            threshold=quantile(abs(pca3$loadings), 0.999))
# Get their locus names
gl3$loc.names[which(quantile(abs(pca3$loadings))>0.999)]

threshold<-quantile(abs(pca3$loadings),0.999)

gl3$loc.names[which(abs(pca3$loadings)>threshold)]

gl3$loc.names[which(quantile(abs(pca3$loadings),0.999)>0.0770)]
```



​	

------

<div id='id-section22'/> 

### Page 22: Notes from class commands 2017-04-05; Gene annotation and enrichment   

Gene annotation:   

Most of this has beeen done already by Melissa; we will be walking through what has been done    

It's good to BLAST to both NR database and Uniprot because:    
1) Uniprot can do alot but it is more curiated therefore has less genes to match too   
2) NR has more (genes/protiens?)   

NR takes a while so Melissa started running it but also ran in DIAMOND which goes much faster   
Here is a diagram showing what was done:   

**INSERT DIAGRAM HERE**   

You do this on a **server** (not on a web browser)    
Therefore we install the BLAST+ programs on our server, download and format the NR and UniProt databases, and run BLAST on our server!   

Once downloaded we run the following command:   

```
$ makeblastdb -in uniprot_sprot.pep -dbtype prot
```

The following is where you actually run your files to annotate them:   

**BE SURE TO START A SCREEN** since this will run for a while   

```
#!/bin/bash

# This single line using the blastp command below will compare your transcript fasta file
# (-query) to the already formatted uniref90 database (-db).
# You can enter 'blastp --help' for a list of the parameters.
# We choose the tab-delimited output format (6) and to only help the top hit (-max_target_seqs)
# and only if it has a minimum evalue of 0.00001.

blastp -query /data/project_data/assembly/08-11-35-36_cl20_longest_orfs_gene.cds \
       -db /data/project_data/assembly/database/uniref90/uniprot_sprot.pep \
       -out /data/project_data/assembly/blast/blastp_vs_uniprot.outfmt6 \
       -outfmt 6 \
       -evalue 1e-5 \
       -max_target_seqs 1
```
The codes she used for each database she compared to can be found in:    

```
data/scripts/
ll
```
The scripts include (but are not limited to:   

```
blastp_nr.sh
blastp_Pm.sh 
blastp_uniprot.sh 
```

To make a master table: There is code but it isn't posted   
* it uses a series of merge commands; didn't talk too much about it today    

Make annotation table: more stuff we don't know...    

Today we are going to work on funcional enrichment analysis   

To do so we need to move EVERYTHING from here   

```
/data/project_data/enrichment
```
into a folder on your desktop    

Open the "GO_MWU_class.R" script in R studio    

set your working directory to the folder you just created and dumped everything into    

We ran lines 23 - 35 but ran into issues.  Will troubleshoot and try again (later)   

UPDATE:   
It was an issue with PCs because the we did not have perl built in.   
we downloaded perl, restarted the computer, re-ran the script and it worked!   
I'm not sure what it means but here are the "results"   

```
go.obo annotation_table results_int_H0vsS1_neglogpval BP largest=0.1 smallest=5cutHeight=0.25

Run parameters:

largest GO category as fraction of all genes (largest)  : 0.1
         smallest GO category as # of genes (smallest)  : 5
                clustering threshold (clusterCutHeight) : 0.25

-----------------
retrieving GO hierarchy, reformatting data...

-------------
go_reformat:
Genes with GO annotations, but not listed in measure table: 422

Terms without defined level (old ontology?..): 0
-------------
-------------
go_nrify:
1724 categories, 2226 genes; size range 5-222.6
	30 too broad
	1006 too small
	688 remaining

removing redundancy:

calculating GO term similarities based on shared genes...
311 non-redundant GO categories of good size
-------------

Secondary clustering:
calculating similarities....
Continuous measure of interest: will perform MWU test
0  GO terms at 10% FDR
```
------

<div id='id-section23'/> 

### Page 23: Notes from class commands 2017-04-10; 16s data analysis (Part 1)  

You can find the tutorial to follow along here: https://adnguyen.github.io/2017_Ecological_Genomics/Tutorial/2017-04-10_16sAmipliconSeqData.html     

There were 173 (ish) samples    
36 sea stars and 6 time points (they didnt' all live all six days)    
​    
We should start by making a diretory to store all the files related to this section:    

```
mkdir 16s_analysis
cd 16s_analysis
```

We will use QIIME to make an OTU table     

This program will:    
1) BLAST your sequences to a reference data based (we will use GreenGenes) to see what it is (if it is annotated)    
2) Group the ones that don't match to known taxa based on similarities into OTUs    
​    
We should be able to call everything in our home directory (and it should call anything it needs from the main directory)    

Mapping file is really imporatant because it takes the file name and give it meaning (it includes all info about the sequences: health status, date collected etc)    
​    
NOTE: there are two map files:    
* map.txt (used for QIIME)    
* R_map.txt (use for R)    

Go to file for QIIME and open it to look at it:    
```
cd /data/project_data/16s 
vim map.txt
:set nowrap
```

Phenotype is if the sample was sick or healthy at time she took the sample     
the number is how sick it was    
* 0 =healthy     
* 1 2 3 4 = diff levels of sick     
* 5 = dead     

Now we will do something in our home directory:    
​    
```
validate_mapping_file.py -m /data/project_data/16s/map.txt -o validate_map -p -b    
```
If you get the following message its ok    

```
Errors and/or warnings detected in mapping file.  Please check the log and html file for details.
```

Oops I was supposed to run the above command IN the new directory but i ran it in my general directory so I'll move it:    

```
mv validate_map/ ~/16s_analysis/
```

I checked and it was there so we are good to go.    

Now use WINSCP to open and look at the following file:    

```
map.html 
```

drag and drop that file to your desktop (or wherever to look at it) and open it in a browser    

It should be a table of that data (I think)    

There are raw data files on the server that we will start working with but then will will skip to the final files because it takes ~4 days to run     


These files are in... but we will run the command in our own directory     

You can tell they are paired end samples because they have 2 reads (R1, R2)    

run this command:    
```
multiple_join_paired_ends.py -i /data/project_data/16s/data_files -o ~/16s_analysis/joined --read1_indicator _R1 --read2_indicator _R2
```

Comments in the meantime:    

QIIME is a list of TONS of python scripts    
* when you install it, it installs everything you need to run each scripts    
* when you click the script on the website it will tell you what it needs     
* its been around for a while and there are tons of support (online chat rooms etc)    
* This is NOT the only program to make an OTU table but it is the easiest to use.     

Reads at this point:    
- they are straight from sequencing facility     
- the facility did de multiplex (remove barcodes etc)     

QIIME can do it all:    
- demultiplex -> creating OTU table     

OK now the command is done running so check your directory to see what it did:    
​    
```
ll #in the 16s_analysis directory 
```

There is now a "joined" directory     
go into joined and you should see 1 directory for each sample (all should have _R1 at the end)     

```
cd joined/
ll
```
YUP! good to go!    

We need the file names in joined to match PERFECTLY to the mapping file so we need to edit the file names to make them match the mapping file     
We need to:    
* remove the _ after the first number    
* remove _R1     

Melanie created python scripts to help remove these:     

So within "joined" directory run these commands one at a time:     

```
bash /data/project_data/16s/remove-underscore.sh
bash /data/project_data/16s/remove-R1.sh
ll 
```
Now you can see that it removed the _ and R1     

Now we will run the following command:    

```
multiple_split_libraries_fastq.py -i ~/16s_analysis/joined -o ~/16s_analysis/filtered -m sampleid_by_file --include_input_dir_path --remove_filepath_in_name  --mapping_indicator ~/16s_analysis/map.txt
```
This will also demultiplex but it was already done sooooo we told it to include_input_dir_path    
* this tells it all of these files will be named exaclty the same thing so take the name of the directory     

--mapping_indicator tells it to use the map file     
She accidently...     

Edit her script to say: "--mapping_indicator /data/project_data/16s/map.txt"    

```
multiple_split_libraries_fastq.py -i ~/16s_analysis/joined -o ~/16s_analysis/filtered -m sampleid_by_file --include_input_dir_path --remove_filepath_in_name  --mapping_indicator /data/project_data/16s/map.txt
```

-o means output    
* it will create the filtered directory     

Run the above (edited command)    


While this ran we talked about the BIG PROJECT:    
* the last two class sessions are open sessions where we come in and can get help     
  * there is this week and next week then the final week is the open sessions    
* sit down with groups ahead of those sessions to decide what we want to do     
* the following week (May1st) is when we will present our final presentation (group) and the following week the write up is due (individual)     
* Syllabus has guidelines for write up (written in molecular ecology style (check website for specific instructions); page length = 4500 words, combined total 5 tables and figures)    
* Presentation guidelines:    
  * 15min    
  * provide background to motivate hypothesis    
    system    
    disease    
    and how it motivates what we are testing    
  * discussion of analysis pipeline (main methods)    
  * presentation of results and a discussion on them    
  * can be an outline for the write up     
  * can split with people in group     
  * similar structure to our assignments     
  * We can used slide (powerpoint)     
* if we come across new methods we want to try (that we came across in papers etc) we can mention it to Steve and Melissa and they will try to help     

NOw that the command is done you should have a filtered directory (go check):    

```
cd ~/16s_analysis/
ll
```
Its there!    

Next we will do a check before we run the command that runs for 4 days to try and eliminate errors     

First lets check the file called "seqs.fna" in the filtered directory:    

```
cd filtered/
ll
head seqs.fna 
```

We are testing that our file has data in it so we are extracting data from the file into a "test" file (which we will later delete) to see if it extracts anything:    

```
extract_seqs_by_sample_id.py -i seqs.fna -o test -s 04-5-05
```

head the "test" file and it should be all "04-5-05" files     

```
ll
head test 
```
Good!    

Now lets remove the file becuase we don't need it:    

```
rm test
```

Now we will make the table:    
there are a few ways to do this:    
* open reference OTU picking    
  * no data base; uses sequence reltaedness to each other     
  * great but extreamly computationally intensive; better to do with a smaller data set     
* closed OTU picking    
  * uses a database    

Currently  the "pick_open_reference_otus.py" is the recommended one to use    

we will first run closed then run open (I think thats what she said)


This command take 4 days, we will test the file and let it run to see if any errors then kill the command     

```
cd ~/16s_analysis
pick_open_reference_otus.py -i ~/16s_analysis/filtered/seqs.fna -o ~/16s_analysis/otus  --parallel --jobs_to_start 1
```

Count to 10 then cancel the command by hitting "Ctrl C"    

remove the file now because we didn't let it run all the way:    

```
rm -r otus 
```

She ran the full command earlier so lets go look at it (no need to copy it just look):    

```
cd /data/project_data/16s/otu_table
ll
biom summarize-table -i /data/project_data/16s/otu_table/otu_table_mc2_w_tax_no_pynast_failures.biom
```

The one with the longest name is the one to look at (because the most things have been done to it)    

Running the "biom summarize table" is what you run to check for errors     
What we want to see (important numbers):    
* number of samples: 176    
* Number of observations: 93033 # of OTUs; its alot and we will filter them later     

Indicators of if errors occured:    
* anything other then the # of individuals we started with    
* if the other numbers (num observations, etc) are low     
  * low is subjective but look at lit to see what is acceptable     


Next class we will filter the OTU table and maybe even start working with the filtered data set in R    
​    
HOMEWORK: get phyloseq up and running in R before next class (google how to do it)     
* install package and see that its running    

We had some time in class so we installed it in class     

------

<div id='id-section24'/> 

### Page 24: Notes from class commands 2017-04-12; 16s data analysis (Part 2)  

**Filtering OTU table**   

Last time: created OTU table (with about 93000 OTUs)   
Today: filter table    

**1) We will filter chimeric sequences**   
* a combination of 2 (PCR) sequences    
* not common with larger sequencing sets; more common with amplicon sequencing   
* done with homology to a data base (green genes); compares    
* we will use Usearch (free for small data sets, $$$ for larger data sets); Bsearch is basically a copy of Usearch but its free!   

```
vsearch --uchime_ref /data/project_data/16s/otu_table/rep_set.fna --chimeras ~/16s_analysis/mc2_w_tax_no_pynast_failures_chimeras.fasta --db /usr/lib/python2.7/site-packages/qiime_default_reference/gg_13_8_otus/rep_set/97_otus.fasta
```

-- uchime_ref algorithum that usearch came up with; saying to use this method; searches green genes database to see if first half = this gene second half = this gene therefore its a chimera and it will remove it    
**ADD NOTES ABOUT THE OTHER PARTS OF THE ABOVE COMMAND    
-- db is where your database is located    

The above command makes the fasta file with all the chimeras; the next step will remove them    

**2) Remove the chimeric OTUs from our OTU table**   

.py is a python script that contains the commands nessesary to remove the chimera files    
-i (input) is the OTU table she gave us last class    
-o (output) the output will go to whereever you run the command to make sure you run this command in the directory you want the data to go (16s_analysis)   

```
filter_otus_from_otu_table.py -i /data/project_data/16s/otu_table/otu_table_mc2_w_tax_no_pynast_failures.biom -o otu_table_mc2_w_tax_no_pynast_failures_no_chimeras.biom -e ~/16s_analysis/mc2_w_tax_no_pynast_failures_chimeras.fasta
```

**3) Remove these chimeric OTUs from rep_set_aligned and re-make the phylogenetic tree**   

This command removes the chimeras from the "rep_set_aligned" file:   
```
filter_fasta.py -f /data/project_data/16s/otu_table/pynast_aligned_seqs/rep_set_aligned_pfiltered.fasta -o ~/16s_analysis/rep_set_aligned_pfiltered_no_chimeras.fasta -a ~/16s_analysis/mc2_w_tax_no_pynast_failures_chimeras.fasta -n
```

The next command makes the tree    
The below command will take a while so run in screen    
```
screen 
make_phylogeny.py -i ~/16s_analysis/rep_set_aligned_pfiltered_no_chimeras.fasta -o ~/16s_analysis/rep_set_no_chimeras.tre
"Ctrl" a d #gets you out of screen but lets command still run 
```


**Frequency Filtering:** gets rid of low frequency OTUs    
* will remove OTUs with fewer then 50 total counts across sall samples (–min_count 50)   
* Will also remove any OTU that is in fewer than 25% of samples (–min_samples 44)   

```
filter_otus_from_otu_table.py -i otu_table_mc2_w_tax_no_pynast_failures_no_chimeras.biom -o otu_table_mc2_w_tax_no_pynast_failures_no_chimeras_frequency_filtered.biom --min_count 50 --min_samples 44

##How many OTUs are left?
biom summarize-table -i otu_table_mc2_w_tax_no_pynast_failures_no_chimeras_frequency_filtered.biom
```
Number of OTUs are now 1064 (num observations)    

This is on par for what is seen in the literature   

Each line is a seperate individual with the number of reads (18000 is the min)   


**Core Diversity analyses in QIIME**   

This script runs a number of diversity indicies on your data    
-o (output) this will be a directory of files which you will move to your desktop so you can look at the files and the webpage it generates   
-e its a kind of normalization (she has a nice description on the tutorial)   
* our -e is at 20,000 which means if they have more then 20,000 reads they will pick random samples from that +20,000   

NOTE: we will not run this command because it takes a few days   
We can click the link in the tutorial to see the webpage it generated when Melanie ran it    
http://www.uvm.edu/~mlloyd/mc2_w_tax_no_pynast_failures_no_chimeras_frequency_filtered_core_diversity/    
* weighted takes into account the...   
* unweighted takes into account the presence or absence of certain OTU results   


```
core_diversity_analyses.py -o core_diversity_filtered -i otu_table_mc2_w_tax_no_pynast_failures_no_chimeras_frequency_filtered.biom -m ~/Po/MiSeq/joined/map.txt -t rep_set_no_chimeras.tre -e 20000 -a 8
```

We will get phyloseq up and running and work through bugs so we can jump right in next time    

***move into R studio***   

You will need: 4 files   
* otu_table_mc2_w_tax_no_pynast_failures_no_chimeras_frequency_filtered.biom #in your own 16s_analysis folder   
* rep_set_no_chimeras.tre #move when done; will be in your own 16s_analysis folder    
* R_map.txt #in /data/project_data/16s   
* phyloseq_script.R #in /data/project_data/16s   

Once all the files are moved open the R script in R studio and load the libaries to see if they work (run lines 1-5)   

If everything runs ok then import the OTU table (run lines 8-10)   

NOTE: warnings are normal    

Run all lines 1-40 by the end of class    

------

<div id='id-section25'/> 

### Page 25: Notes from class commands 2017-04-17; 16s data analysis (Part 3)  

We will be working in R today:   

* Open R studio   
* open the "phyloseq_script.R"   
* set the working directory to the correct folder   
* run commands line 1-22   

**line 8:** assigns everything that is needed for making an otu table to that command; converts .biom file to a readable format in R    

OTU table is NOT normalized at this point; it will get normalized as we go along   

Head and tail the table to see what it looks like:   

```
head(otu_table(phylo))
tail(otu_table(phylo))
```

when we ran a head we saw only numbers on the left cloumn (those are assigned if close picking was done)   
when you run the **tail** you see the rows are called "New.CleanUp.ReferenceOTU6178058" which is assigned when open picking was done.   

Next we will test for differentially expressed OTUs    
* wrapper for DESeq2; takes data from a different format and make it accessable in a different format    

For simplicity sake we only want to compare sick and healthy (not dead) so we will remove the dead samples by running line 32   

**line 35-36:** we want to change any integers (#1) to a name (name 1)   

**line 39:** the last thing you put model is what it will compare   
* phenotype is at the time of sampling (not final phenotype)    
* this controls for the repeated measure of individual(?)   

**line 42:** takes some time    

While its running she will show us the resources she used to get here (resources can be found at the top of her tutorial):   
* QIIME gitbook https://www.gitbook.com/book/twbattaglia/introduction-to-qiime/details    
* QIIME tutorial for working with Illumina data: http://nbviewer.jupyter.org/github/biocore/qiime/blob/1.9.1/examples/ipynb/illumina_overview_tutorial.ipynb    
* phyloseq: http://qiime.org/scripts/compare_categories.html    
  * use the tutorials; super helpful; they provide data sets which can help narrow down if issue with package/command or with your own data.    

NOTE: the QIIME scripts website updates REALLY fast but they don't list if a script is outdate; the gitbook is helpful for figuring that out    

She used "compare_categories" to help figure out what she needed to do http://qiime.org/scripts/compare_categories.html    

**Alpha diversity** is the diversity in *one sample*; basically species richness; # of unique OTUs a species has   

**Beta diversity** is the *between samples* component of diversity.    

The command is done running so now we will look at the results by running lines 45-49   

**line 56:** make a table of all the significant values; you can set the p value to what you want in line 52   
Run lines 52-57   

**Line 60:** write the table we just made to our computer so we can open it up and look at the ones that were diff expressed    
* NOTE: at this point it shows the ones that differ in **abundance** not taxa    

**Lines 63-75:** makes a plot; run them all together    
* this shows you what taxa were diff sig expressed    
* each point is one OTU that is diff expressed between sick and healthy   
* log fold change is how diff expressed they are    

If you are ever not sure about which is compared to which you run "head"   

since it says sick vs healthy you know:    
* pos = up in sick    
* neg = down in sick   


Next we will look at particular OTUs of interest and make a plot; choose individual otus from list of significantly diff otus generated and put into "pheno_sigtable"   

Run lines 78-82   
* change otu you want to run by changing "New.ReferenceOTU2814" on line 78 and 79    

OTU 2814 is up reg (aka more ind.) in healthy and less in sick    

   

Now we will start testing for diff expression ***between samples***   

We will start with looking at OTUs that are change in # as an indv changes from healthy -> sick -> more sick -> dead    
* to look at change in health status we run "pheno_num"   

**Lines 88 to line 89** does same as above, take integer and gives it a name    

**Line 92:** sets "pheno_num" as the variable you are comparing   
* since individual is lister first it means we are taking into account that individuals were sampled multipule times over time (right?)   

**Line 95:** runs the DESeq2; will take a while (20min?)   

When you run the head command (line 99) you should see what that table is comparing by looking at "wald test p-value" (should say 5 vs 0)   

The "wald" test is a chi square test comparing the level of expression (or in this case OTUS0 between your two categories of interest with the null hyp that the two groups are the same   
* sig p values means that the test stat was higher then expected; there was a true diff between the groups   
* Melanie was advised to use this test in the tutorial    

She as written out how to compare healthy to S_1 -> S_5 but we might not so this in class and do a plot instead    

We decided to do the graph so that we can move on from phyloseq starting next class   

So **skip down to line 133**   

Rarefy helps with reproducability; do this in **line 135 and 136**    

**Lines 139-141:** check that we rarified correctly by generating a plot where all the samples should have the same number of reads   

Next we will make a graph that shows the relative abundance of the taxa in the ones, in the twos, in threes...   
we want the hight of the bars to all be the sample; thats what line ___ does    

run lines 145-150 together   

NOTE: we had an issue with color brewer but the general graph is correct    

------

<div id='id-section26'/> 

### Page 26: Notes from class commands 2017-04-19; 16s data analysis (Part 4); PIECRUST  

**PICRUST** allows you to get at some of the info you would get when doing whole genome sequencing without having to do whole genome   

* uses an extended ancestral-state reconstruction algorithm to identify a closely related microbe with known full genome sequence to each OTU   
  * takes your...   

Has NSTI (nearest sequenced_____)   

gives you an OTU type table in a similar format (.biom); counts table   

Big limitation:   
* can only work with an OTU that is a closed references OTU; the picking that uses a reference database (aka the best understood OTUs)   

if OTU has just a number = not with closed picking   
any work in front of the OTU = picked with closed (check)   

The majority of ours were NOT picked with the closed reference so our first step is to pick with the closed reference (closed OTU table)   

NOTE: can run from home directory or the directory where the files are because this tells it where to go:   

```   
filter_otus_from_otu_table.py -i ~/16s_analysis/otu_table_mc2_w_tax_no_pynast_failures_no_chimeras_frequency_filtered.biom -o ~/16s_analysis/closed_otu_table.biom --negate_ids_to_exclude -e /usr/lib/python2.7/site-packages/qiime_default_reference/gg_13_8_otus/rep_set/97_otus.fasta 
```

we now have 259 OTUs (low number)      

normalize by picrust’s method:   
* normalizes by copy number

```   
normalize_by_copy_number.py -i ~/16s_analysis/closed_otu_table.biom -o ~/16s_analysis/closed_otu_table_norm.biom   
```

Now we will do what we came here to do; predict metagenomes    

The output of PICRUST is the same format as an OTU table but instead of OTUS, KEGG Orthology terms are used. For a tab delimited output:   

```
predict_metagenomes.py -f -i ~/16s_analysis/closed_otu_table_norm.biom -o ~/16s_analysis/metagenome_predictions.txt -a nsti_per_sample.txt 
```
-f means to put in tab delimited format    
-a we want you to calc thes ____ numb...(CHECK)   

Below is the same but the format is different (no -f); we will be able to look at the one below but we can't look at the one above   

```
predict_metagenomes.py -i ~/16s_analysis/closed_otu_table_norm.biom -o ~/16s_analysis/metagenome_predictions.biom -a nsti_per_sample.txt   
```

Now we can look at the file generated from the above command:   

```   
head metagenome_predictions.txt   
```

each row is a KO term; if we count the number of rows we will know the number of terms that got prediceted so we will do that next:   

```
wc -l metagenome_predictions.txt   
```

6,910 KO terms/rows   
there are more ko terms then OTUs; there are large number of KO terms per OTU    


Collapse to higher KO hierarchy term    

```
categorize_by_function.py -f -i metagenome_predictions.biom -c KEGG_Pathways -l 3 -o metagenome_predictions.L3.txt   
```

We also want this output in .biom format to use in R    

```
categorize_by_function.py -i metagenome_predictions.biom -c KEGG_Pathways -l 3 -o metagenome_predictions.L3.biom   
```

Move metagenome_predictions.L3.biom to your desktop to use in R; use WINSCP   


Move into R and copy and paste the script from the tutorial into an new R script; saved script as "PIECRUST.R"

```
library("phyloseq"); packageVersion("phyloseq")
library("DESeq2")
packageVersion("DESeq2")
library("ggplot2")
theme_set(theme_bw())
library('biom')

x = read_biom("metagenome_predictions.L3.biom")
otumat = as(biom_data(x), "matrix")
OTU = otu_table(otumat, taxa_are_rows=TRUE)


mapping <- import_qiime_sample_data(mapfilename = 'R_map.txt')

phylo <- merge_phyloseq(OTU, mapping)
phylo

###############################################################################
###Test for DE KO terms between individuals that got sick and those that didn't
###############################################################################

final_pheno = phyloseq_to_deseq2(phylo, ~ Final_phenotype)
final_pheno_results = DESeq(final_pheno, test="Wald")
final_pheno_res = results(final_pheno_results)
summary(final_pheno_res)
head(final_pheno_res)

alpha = 0.05
final_pheno_sigtab = final_pheno_res[which(final_pheno_res$padj < alpha), ]
final_pheno_sigtab= cbind(as(final_pheno_sigtab, "data.frame"), as(tax_table(phylo)[rownames(final_pheno_sigtab), ], "matrix"))
head(final_pheno_sigtab)
final_pheno_sigtab
write.table(final_pheno_sigtab, "Final_pheno_L3.txt", sep="\t")
```

Load all libraries (line 1-6)   

ERROR when loading the biom library    
* this took a while to troubleshoot so we ended up following along on her screen    
  * see audio in onenote   

WAIT: they found a solution: (this has been added to script)   

```
install.packages("RJSONIO")   
```
once that is done running go to:   

https://cran.r-project.org/src/contrib/Archive/biom/   

and download the newest: biom_0.3.12.tar.gz   
* I put it in the same folder where we are working out of   

go back to R and run this:   

```
install.packages("~/OTU r scripts and files/biom_0.3.12.tar.qz", repos=NULL, type="source")   
```
ERROR invalid package   

nope never mind still taking too long so she ran it on the screen:   

lines 24-28: similar to phyloseq deseq we ran the other day.   

lines 30-35: build the table and output it to your computer    





------

<div id='id-section27'/> 

### Page 27: Script for FINAL PROJECT 2017-05-03; DESeq2 and Gene annotation.   

Code use to run my part of the Final Project:

```
setwd("C:/Users/Hannah/Desktop/Ecological Genomics class/DGE data from 2-27")   
library("DESeq2")   

library("ggplot2")   

countsTable <- read.delim('countsdata_trim2.txt', header=TRUE, stringsAsFactors=TRUE, row.names=1)   
countData <- as.matrix(countsTable)   
#head(countData)   

conds <- read.delim("cols_data_trim.txt", header=TRUE, stringsAsFactors=TRUE, row.names=1)   
head(conds)
colData <- as.data.frame(conds)
#head(colData)

###########Code for Objective 2 can be found on Lisa Chamberland's git hub######################

##Code for Objective 1
#Subsampling for Objective 1 (sick)

#SSS (28); Day 3, Day 6, Day 9
#count
count_SSS1 = countData[,43:45]

#col
col_SSS1 = colData[43:45,]

#SSS (29); Day 3, Day 6, Day 9
#count
count_SSS2 = countData[,46:48]

#col
col_SSS2 = colData[46:48,]

#SSS (36); Day 3, Day 6, Day 9
#count
count_SSS3 = countData[,71:73]

#col
col_SSS3 = colData[71:73,]

#SSS (08); Day 9, Day 12, Day 15
#count
count_SSS4 = countData[,6:8]

#col
col_SSS4 = colData[6:8,]

#SSS (09); Day 9, Day 12, Day 15
#count
count_SSS5 = countData[,10:12]

#col
col_SSS5 = colData[10:12,]

#Now we will merge the col_SSS files together and the count_SSS files together
#we use rbind for col data since the ID is a row
col_SSS <- rbind.data.frame(col_SSS1,col_SSS2,col_SSS3,col_SSS4,col_SSS5)
dim(col_SSS)

#we use cbind for count since ID is a column
count_SSS <- cbind.data.frame(count_SSS1,count_SSS2,count_SSS3,count_SSS4,count_SSS5)
dim(count_SSS)


#Subsampling for objective 3 (healthy)
#col
col_HHH_1 = colData[13:15,]
col_HHH_2 = colData[34:36,]
col_HHH_3 = colData[40:42,]
col_HHH_4 = colData[49:51,]
col_HHH_5 = colData[58:60,]
col_HHH_6 = colData[63:65,]

#merge col tables
col_HHH_total <- rbind.data.frame(col_HHH_1, col_HHH_2, col_HHH_3, col_HHH_4, col_HHH_5, col_HHH_6)

dim(col_HHH_total)

#count
count_HHH_1 = countData[,13:15]
count_HHH_2 = countData[,34:36]
count_HHH_3 = countData[,40:42]
count_HHH_4 = countData[,49:51]
count_HHH_5 = countData[,58:60]
count_HHH_6 = countData[,63:65]

#merge count tables
count_HHH_total <- cbind.data.frame(count_HHH_1, count_HHH_2, count_HHH_3, count_HHH_4, count_HHH_5, count_HHH_6)

dim(count_HHH_total)

#Now we need to combine the count healthy and sick files and the col healthy and sick files
#count
count_obj3 <- cbind.data.frame(count_HHH_total,count_SSS)

dim(count_obj3)

#col
col_obj3 <- rbind.data.frame(col_HHH_total,col_SSS)

dim(col_obj3)

#Now we can start the time course DESeq2 command (model 1 from 2/27/17)
#we are having issues with using indiv as a control so for now we will use location as a control
ddsobj3 <- DESeqDataSetFromMatrix(countData = count_obj3, colData = col_obj3, design = ~ location + health)
ddsobj3 <- ddsobj3[ rowSums(counts(ddsobj3)) > 100, ]

dim(ddsobj3)

ddsobj3 <- DESeq(ddsobj3, parallel=T)
resobj3 <- results(ddsobj3)
resobj3$symbol <- mcols(ddsobj3)$symbol

head(resobj3[order(resobj3$padj),],4)
summary(resobj3)


#Add code for: volcano plot, PCA, and venn diagram ##############################
#volcano plot
which(resobj3$padj <0.05)
sig.genes.health <- resobj3[which(resobj3$padj <0.05),]
head(sig.genes.health)

with(resobj3, plot(log2FoldChange, -log10(pvalue), pch=20, main="Differential gene expression of HHH vs SSS", xlim=c(-5.5,5)))

points(sig.genes.health$log2FoldChange,-log10(sig.genes.health$pvalue), col='red', pch=20)


#code for PCA plot
vsd.h <- varianceStabilizingTransformation(ddsobj3, blind=FALSE)

p.h <- plotPCA(vsd.h, intgroup=c("health"))

p.h +ggtitle("PCA plot of HHH vs SSS") +theme(plot.title = element_text(hjust = 0.5), axis.title = element_text(size=12), axis.text = element_text(size=12))





#################Functional enrichment####################################
#We need to take the results from DEseq2 and make a table with two columns: gene ID and LFC 
#Therefore we need to export the results file:
#write.csv(resobj3, file = "Resultsobj3.csv", quote=F, row.names=T)

#now we bring all the files we need into R:

input="Resultsobj3_edited3.csv" 
goAnnotations="annotation_table" 
goDatabase="go.obo" 
goDivision="BP" 
source("gomwu.functions.R")


# Calculating stats. It takes ~3 min for MF and BP. Do not rerun it if you just want to replot the data with different cutoffs, go straight to gomwuPlot. If you change any of the numeric values below, delete the files that were generated in previos runs first.

gomwuStats(input, goDatabase, goAnnotations, goDivision, perlPath="perl", largest=0.1, smallest=5, clusterCutHeight=0.25)

#Nothing was functionally enriched so we will manually look at the DE genes indentified in DESeq2 to see what their funtions are. See code below on how this was done.

##########################DE gene gene annotation##############################

#First we take the results file from DESeq2 and take out the genes that were significant (<0.05)
#we already generated an object that contains all the gene IDs that are significant: sig.genes.health
# start by reading in the FULL annotation table:
GOterms <- read.table("poch_uniprot_GO_nr.txt", header=TRUE, sep = "\t")

#Check how many genes are significant:
summary(sig.genes.health)

#Now we will pull out the significant GO terms
GOtermssig <- GOterms[GOterms$trinID %in% row.names(sig.genes.health),]

#Since there are around 205 genes that are sinificant focus on reporting: 1) the # of sig 2) only report the function of the top %
#to do this change the sig level set when setting sig.genes.health:
sig.genes.health.top <- resobj3[which(resobj3$padj <0.0001),]
summary(sig.genes.health.top)

GOtermssig_0001 <- GOterms[GOterms$trinID %in% row.names(sig.genes.health.top),]

#To export this file we use the following command:
write.csv(GOtermssig_0001, file = "top20siggenes_GO.csv", quote=F, row.names=T)

```

------

<div id='id-section28'/> 

### Page 28:  

------

<div id='id-section29'/> 

### Page 29:  

------

<div id='id-section30'/> 

### Page 30:  

------

<div id='id-section31'/> 

### Page 31:  

------

<div id='id-section32'/> 

### Page 32:  

------

<div id='id-section33'/> 

### Page 33:  

------

<div id='id-section34'/> 

### Page 34:  

------

<div id='id-section35'/> 

### Page 35:  

------

<div id='id-section36'/> 

### Page 36:  

------

<div id='id-section37'/> 

### Page 37:  

------

<div id='id-section38'/> 

### Page 38:  

------

<div id='id-section39'/> 

### Page 39:  

------

<div id='id-section40'/> 

### Page 40:  

------

<div id='id-section41'/> 

### Page 41:  

------

<div id='id-section42'/> 

### Page 42:  

------

<div id='id-section43'/> 

### Page 43:  

------

<div id='id-section44'/> 

### Page 44:  

------

<div id='id-section45'/> 

### Page 45:  

------

<div id='id-section46'/> 

### Page 46:  

------

<div id='id-section47'/> 

### Page 47:  

------

<div id='id-section48'/> 

### Page 48:  

------

<div id='id-section49'/> 

### Page 49:  

------

<div id='id-section50'/> 

### Page 50:  

------

<div id='id-section51'/> 

### Page 51:  

------

<div id='id-section52'/> 

### Page 52:  

------

<div id='id-section53'/> 

### Page 53:  

------

<div id='id-section54'/> 

### Page 54:  

------

<div id='id-section55'/> 

### Page 55:  

------

<div id='id-section56'/> 

### Page 56:  

------

<div id='id-section57'/> 

### Page 57:  

------

<div id='id-section58'/> 

### Page 58:  

------

<div id='id-section59'/> 

### Page 59:  

------

<div id='id-section60'/> 

### Page 60:  

------